[{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Automating Budget Management Across Multi-Account Environments Managing AWS spending across multiple accounts requires a sophisticated approach to budget control and monitoring. Our custom solution enables centralized budget management with automated email notifications, allowing organizations to set and enforce account-specific budgets from a central management account. This automated system tracks spending across individual accounts and sends timely alerts when accounts approach or exceed their allocated budgets. The central management account serves as the single source of truth, where financial teams can configure budget thresholds for each account and receive notifications about spending patterns across the organization\u0026rsquo;s entire account ecosystem.\nSolution Overview Our solution implements an event-driven architecture to automate budget management across the entire AWS Organization. The process begins at the management account, where individual account budgets are defined and stored in an Amazon DynamoDB table. AWS Lambda functions automatically propagate these budget configurations to each account through their corresponding AWS Systems Manager (SSM) Parameter Store. AWS Budgets monitors spending in real-time, triggering email notifications when accounts approach or exceed allocated thresholds. This streamlined architecture eliminates manual budget management while providing teams with flexibility to operate within defined cost parameters.\nSolution Architecture Solution Components AWS Control Tower Management Account:\nDynamoDB Table stores budget information for each linked account. Lambda Function triggered by DynamoDB Table updates and proceeds to update budgets in spoke accounts through AWS Systems Manager. Linked Account:\nAWS SSM parameter store holds the updated budget values. Amazon CloudWatch events trigger automated processes when parameters (AWS SSM parameter) are updated. AWS Budgets manages account costs based on these parameters. AWS Budgets sends email notifications if budget thresholds are exceeded. Workflow: Upgrade DynamoDB table: Users update budget values for a linked account in the DynamoDB table (BlogBudgetsDynamoDB) located in the management account section.\nDynamoDB Stream Trigger: This update triggers a DynamoDB Stream, which in turn triggers Lambda function (BlogBudgetsUpdateLambda) in the management account section.\nLambda function updates AWS SSM Parameter: Lambda function reads the updated budget value from the DynamoDB table, assumes the cross-account role (BlogBudgetsSpokeRole), and updates SSM Parameter Store (/BlogBudgets/CostThreshold) in the spoke account with the new budget value.\nAmazon EventBridge rule triggers event: When SSM Parameter Store in the spoke account is updated, an EventBridge Rule (BlogBudgetsSSMTrigger) triggers AWS SSM Automation document (BlogBudgetsAutomationDoc) in the spoke account.\nUpgrade Budgets: The AWS SSM Automation document in the spoke account reads the new budget value from SSM Parameter Store and updates the Budgets value (SpokeAccountBudget) accordingly.\nEmail Notification: When spending reaches configured thresholds, the system sends notifications to designated stakeholders.\nOperating the Solution Based on service behavior, when you update the budget value for an account, the budget limit in AWS Budgets will be updated accordingly. However, there are some important points to note regarding alerts:\nEven if you update the budget value after an alert has been triggered, the alert status remains in the \u0026ldquo;Exceeded\u0026rdquo; state (or whatever state it was in when triggered). The alert will not automatically reset just because you changed the budget limit. It may take up to 8 hours for the new budget status to be reflected in the alert system, meaning changes to the budget will not immediately update the alert status. Illustrative Example: Suppose you set a monthly budget of $100, with an alert triggered at 100% of actual spending.\nDuring the month, you overspend and reach $200. The alert will be triggered, and you\u0026rsquo;ll receive a notification that the budget has been exceeded. Even if you increase the budget limit to $250 (to reflect additional costs), the alert will still remain in \u0026ldquo;Exceeded\u0026rdquo; status until the end of the month. This occurs because the alert status is determined based on actual spending and the initial budget limit at the time the alert was triggered, not the updated limit. This behavior ensures you remain aware of all budget overruns while preventing automatic alert resets that could cause you to miss important spending patterns.\nPrerequisites AWS Account Setup AWS Organizations or AWS Control Tower with multi-account structure One management account At least one spoke account Required AWS Services Access AWS Budgets AWS Lambda Amazon DynamoDB AWS Systems Manager (SSM) Amazon CloudWatch AWS CloudFormation Access Permission Requirements Ensure you have the following minimum necessary permissions in your AWS accounts:\nManagement Account:\nAWS CloudFormation deployment permissions DynamoDB table management access Lambda function and CloudWatch logs monitoring permissions Cross-account AWS Identity and Access Management (IAM) roles management permissions Spoke Accounts:\nAWS CloudFormation deployment permissions Read-only access to monitor EventBridge rules, Systems Manager parameters and automation processes, and AWS Budgets Email address to receive budget notifications Detailed deployment steps will be presented in subsequent sections of this article.\nDeploying the Solution 1. Deploy Stack for Management Account In the management account, deploy CloudFormation Stack named \u0026lsquo;budget_mgmt_account.yaml\u0026rsquo;, click here to download this template.\nThis template will deploy:\nDynamoDB table used to store budget values for each spoke account. Lambda function triggered by DynamoDB stream to update SSM Parameter Store (specifically, /BlogBudgets/CostThreshold) in each spoke account with new budget values. AWS IAM roles with necessary permissions for Lambda function, including cross-account access. 2. Deploy Stack for Spoke Accounts You can set up the spoke account stack using one of the following methods:\nMethod 1: Deploy AWS CloudFormation Stack named \u0026lsquo;budget_spoke_account.yaml\u0026rsquo; directly in each corresponding spoke account.\nMethod 2: Use AWS CloudFormation StackSets from the management account to automatically deploy this template to multiple spoke accounts simultaneously. This approach is suitable if you\u0026rsquo;re managing multiple spoke accounts and want to simplify the deployment process.\n3. Update DynamoDB Table After deployment is complete:\nAccess the management account. Navigate to the DynamoDB table named \u0026lsquo;BlogBudgetsDynamoDB\u0026rsquo;. For each spoke account, add a new item: \u0026lsquo;AccountId\u0026rsquo;: Enter the 12-digit spoke account ID, example: 111122223333. \u0026lsquo;BudgetValue\u0026rsquo;: Enter the desired budget value (numeric), example: 1000. When the DynamoDB table is updated with account information and budget values, the Lambda function named \u0026lsquo;BlogBudgetsUpdateLambda\u0026rsquo; will be automatically triggered through the DynamoDB stream to update the new budget value into each spoke account\u0026rsquo;s SSM Parameter Store.\n4. Update SSM Parameter Store After assuming the cross-account role, the Lambda function will update SSM Parameter Store in the spoke account with the new budget value (example: /BlogBudgets/CostThreshold).\nThis step ensures budget information is synchronized to the spoke account and ready for subsequent actions.\n5. Trigger Amazon EventBridge Rule The updated value in SSM Parameter Store will trigger an Amazon CloudWatch/EventBridge rule. This rule monitors changes in the AWS SSM parameter and generates an event each time the /BlogBudgets/CostThreshold parameter is updated.\n6. Trigger SSM Automation Process The CloudWatch event will automatically trigger execution of the SSM Automation document in the spoke account. This document will perform the action of updating the Budgets value based on the new budget parameter value stored in SSM Parameter Store.\n7. Update Budget The AWS SSM Automation document will update the Budgets configuration in the spoke account. This step ensures the new budget value is set in Budgets, enabling the system to accurately track and monitor costs according to the updated budget.\n8. Email Alert Notification When the budget value has been updated in Budgets, the system will check pre-configured alert thresholds. If current spending exceeds this threshold, an email alert will be automatically triggered. This email will notify designated recipients about the budget overrun, helping ensure timely action can be taken to control costs.\nResource Cleanup To avoid future costs, you should delete resources deployed for solution testing purposes by deleting the AWS CloudFormation Stacks deployed in your environment.\nFuture Improvements Trigger Remediation Actions To further enhance AWS cost control capabilities, you can implement automated remediation actions using AWS Budget Actions. This feature allows automatic triggering of cost-adjustment responses when budget thresholds are exceeded. A Budget Action is a feature in AWS Budgets that lets you define automatic cost-saving responses whenever budget alerts are triggered. This ensures responses are executed without manual intervention, while promoting a cost-saving culture in the organization.\nYou can attach remediation actions to budget alerts, configured to trigger when the budget exceeds a certain threshold — whether actual or predicted spending. This is especially useful in preventing unintentional overspending.\nExamples:\nIAM Policy: You can apply the \u0026ldquo;Deny EC2 Run Instances\u0026rdquo; IAM policy to users, groups, or roles when the EC2 budget is exceeded. Target specific instances: You can define actions to stop EC2 instances in a specific region when the EC2 usage budget is violated. This feature helps organizations automate cost-optimization responses based on defined budget thresholds, helping maintain alignment with financial objectives.\nIntegration with ServiceNow for ITSM Process Automation Another future improvement direction is integrating this solution with ServiceNow to seamlessly automate IT Service Management (ITSM) processes. By combining AWS Budgets and remediation actions with ServiceNow, organizations can automatically create tickets or incidents when a budget threshold is exceeded. These incidents can then trigger predefined automation processes in ServiceNow, such as notifying relevant teams or initiating cost optimization processes.\nExamples:\nWhen budget alerts exceed a certain threshold, an incident will be automatically created in ServiceNow to notify the finance or cloud operations team. ServiceNow can automatically escalate the issue to higher-level teams, or initiate remediation actions such as requesting approval to scale down resources or perform cost assessment. This integration helps enhance information flow between finance and operations teams, automates the ticketing process, and ensures budget overrun issues are handled promptly and effectively.\nConclusion This solution provides an automated and scalable method for managing budgets across multiple AWS accounts, leveraging services such as Amazon DynamoDB, AWS Systems Manager Parameter Store, AWS Lambda, and AWS Budgets. The solution helps organizations monitor costs effectively and receive timely notifications when budgets exceed allowed limits.\nFuture enhancements, such as integrating AWS Budget Actions for automated remediation and linking with ServiceNow for ITSM process automation, will strengthen cost management capabilities while improving response speed when budget violations occur. By deploying this solution, organizations can better control AWS costs, maintain spending within budget parameters, and optimize cloud resource utilization efficiency.\nThe solution is currently available at AWS-SAMPLES-REPO\nTAGS: AWS Budget, CFM\nAuthors Gautam Bhaghavatula As Senior Partner Solutions Architect at AWS, Gautam Bhaghavatula leverages deep expertise in cloud infrastructure architecture to design highly scalable solutions and implement robust security measures. His expertise includes systems, networking, microservices, and DevOps, applying AWS\u0026rsquo;s most advanced technologies. Currently, he collaborates with AWS partners to drive cloud migration and modernization projects through strategic direction and deep technical leadership.\nMatt Saeger Senior Deployment Consultant at Amazon Web Services (AWS), leading strategic cloud computing initiatives and orchestrating enterprise-scale transformation programs for customers in the financial services sector. Holding 11 AWS certifications, Matt combines deep expertise in cloud architecture, DevOps, and network architecture with experience leading complex cloud deployment programs. He specializes in building enterprise-level cloud strategies, designing secure and highly scalable solutions, and developing automation solutions with a focus on security and compliance. Contact Matt to discuss cloud transformation strategies, cloud applications in finance, and security best practices in cloud environments.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Introducing: Guidance for a media lake on AWS Many businesses currently struggle with managing large volumes of digital media files scattered across multiple Amazon Simple Storage Service (Amazon S3) buckets. This distributed storage approach creates serious challenges. Finding specific content can be extremely difficult and time-consuming, while organizing files efficiently becomes a complex task, making it challenging to extract full value from existing content.\nMedia teams often have to manually search through multiple storage locations, relying solely on filenames to identify content. This inefficient process slows down production workflows and hinders the ability to reuse media assets effectively.\nOur guidance for building a Media Lake on Amazon Web Services (AWS) shows you how to create a centralized system for searching and managing media assets stored on AWS. This solution provides a user-friendly interface and convenient APIs for finding and managing media files, while allowing you to create a unified catalog for all media assets — even when they\u0026rsquo;re stored across multiple S3 buckets. Importantly, you can achieve this organization without changing your existing storage structure. Our guidance uses automated processes to catalog and process media files, helping accelerate content discovery and utilization more effectively.\nWe\u0026rsquo;ll guide you through how a media lake can help you extract more value from digital content by creating a scalable and searchable media storage system that works seamlessly with AWS services and partner solutions. We\u0026rsquo;ll explain the system design, introduce different ways to set it up, and show how to turn your scattered media files into an organized and easily searchable collection.\nOverview Our guidance for media lake on AWS provides a reference architecture and sample source code that you can deploy to create a media management system. This solution illustrates how to:\nCreate a unified search interface, allowing you to search media files across all Amazon S3 storage locations simultaneously. Build automated event-driven media processing workflows using default and custom pipelines in a drag-and-drop interface. Enable natural language search capabilities through AI-powered integrations. Organize your media files by quickly searching, previewing content, and viewing technical information as well as their descriptions and details. Deployment Media lake uses AWS Cloud Development Kit (AWS CDK) for Infrastructure as Code (IaC) model. You can set up the sample code using one of three methods:\n1. AWS CloudFormation Template Use the provided CloudFormation template to automatically create AWS CodePipeline. This pipeline will use AWS CodeBuild to deploy the AWS CDK application in your chosen AWS account and region.\n2. Local Environment Deployment Deploy media lake directly from your computer using standard AWS CDK deployment methods. This option provides more flexibility, allowing you to change deployment configuration and select AWS profiles before deploying to different AWS Regions as needed.\n3. Continuous Integration and Deployment (CI/CD) Pipeline If you already have an existing CI/CD pipeline used to deploy AWS CDK code, you can integrate media lake into that pipeline, allowing you to deploy media lake through your organization\u0026rsquo;s existing CI/CD pipeline.\nRegardless of which method you choose, all three approaches set up the same set of AWS resources. This flexibility helps you choose the deployment method that best suits your organization\u0026rsquo;s current workflow and needs.\nArchitecture Overview This system provides a user interface and RESTful API built on serverless architecture to ensure scalability and security. Amazon CloudFront distributes static user interface content stored in Amazon S3, enabling fast and reliable global access, while AWS WAF protects the system from common web vulnerabilities and attacks. Amazon API Gateway handles requests, while Amazon Cognito manages user authentication and generates security tokens to access API resources.\nMedia lake stores backend data in Amazon DynamoDB and Amazon OpenSearch Service. Media files, logs, and infrastructure code are stored in Amazon S3. The system manages its workflows using AWS Step Functions, which orchestrates the entire process, while AWS Lambda functions handle the actual work of accessing and managing data in storage systems.\nTo orchestrate media workflow processes used to interact with backend data stores and storage, see the reference architecture for media lake.\nGetting Started When you deploy our system, it automatically creates an administrator account. An invitation email will be sent to the address you provided during the setup process. When logging in for the first time, you\u0026rsquo;ll need to make an important decision about how you want to search your media assets.\nYou have two main options:\nTraditional Metadata Search This method relies on AI-generated keywords, technical metadata, or files to find media assets. This approach is similar to traditional file search systems.\nSemantic Search This advanced feature allows you to use natural language queries to search media. For example, you can type \u0026ldquo;find photos of red racing cars\u0026rdquo; or \u0026ldquo;show me videos of beaches at sunset\u0026rdquo;. This method goes far beyond simple keyword matching — it understands the meaning behind your query.\nIt\u0026rsquo;s important to decide on the search method and configure settings before you start importing media into the media lake. This ensures all media assets are properly indexed from the start, speeding up searches later.\nIf you enable semantic search, traditional metadata search remains available, and you can flexibly switch between the two methods. Figure 1 shows an example of semantic search, displaying the query: \u0026ldquo;mountains with snow on them and a purple background\u0026rdquo;.\nCurrently, to enable semantic search, our guidance system uses TwelveLabs\u0026rsquo; Marengo embedding model for AI-powered video understanding capabilities, combined with OpenSearch Service acting as the vector database. Choose TwelveLabs API as the embedding provider and OpenSearch Service as the embedding storage. Next, configure TwelveLabs integration and import TwelveLabs\u0026rsquo; pre-configured audio, image, and video pipelines. After completing configuration, these pipelines will automatically create embeddings for all supported media loaded, thereby enabling semantic search functionality.\nStorage Connectors This guidance system uses storage connectors to link your existing S3 buckets with the system. These connectors serve two important purposes: First, they automatically process any new media files added to your S3 buckets; Second, they sync existing media files in S3 buckets with your media lake. This creates a unified interface displaying all your media files from multiple storage locations.\nWhen you add a storage connector, the guidance system begins a sequence of steps to integrate your content:\nActivates Amazon EventBridge notifications on the target S3 bucket, if not already enabled. Creates an EventBridge rule to forward Amazon S3 events to Amazon Simple Queue Service (Amazon SQS) queue to ensure stable event processing. When deployed, the system stores the Amazon S3 ingest Lambda function in the IaC S3 bucket. This Lambda function processes queued events to automatically index new media assets. After the Lambda function processes a media asset, an event is sent to the media lake analysis event bus to perform additional analysis and transformations for that media asset. Our guidance currently requires storage connectors to be in the same AWS Region and account as the media lake. After you add supported new media to the S3 bucket after creating the storage connector, the system will automatically detect, index, and enable searching of these media files.\nMedia Lake Integrations Media lake integrations provide secure credential management capabilities used by pipelines to access external services in your media workflows. By separating credentials from pipeline configuration, you can rotate API keys and manage access independently without editing workflows. This approach allows you to update credentials in one place, and that change automatically applies to all pipelines using those credentials. You can use the media lake interface to create, edit, and update integrations for external services.\nThis guidance system uses AWS Secrets Manager to securely store all credentials. When you create a pipeline, the system stores necessary credentials as environment variables through Secrets Manager ARNs (Amazon Resource Names). During pipeline execution, each step requiring access to an external service retrieves the corresponding credentials from Secrets Manager using the stored ARN. This approach provides two main benefits: compliance with AWS security best practices in managing third-party API keys and providing centralized control over all credentials in your media workflows. This method helps manage credentials securely and efficiently throughout the media processing workflow.\nMedia Lake Pipelines This guidance system helps optimize creating media workflows on AWS through the pipeline feature. During deployment, the system reads configuration files from its repository to identify nodes to be created and set up. These nodes then display in the drag-and-drop interface, allowing you to visually design your workflow.\nOnce you\u0026rsquo;ve arranged the workflow and saved the pipeline, the system converts your design into an AWS Step Functions state machine. This state machine orchestrates the media workflow according to the specific configuration you defined.\nTo manage workflow execution, the system sets up three additional components: an Amazon EventBridge rule, an Amazon SQS queue, and an AWS Lambda function. The EventBridge rule is responsible for sending events to the SQS queue, while the Lambda function processes these events from the queue and triggers the corresponding Step Function, starting your media workflow.\nThis mechanism provides a user-friendly approach to building complex media processing workflows while leveraging the power of AWS services behind the scenes.\nWhen deployed, the system automatically deploys three default pipelines: Default Video Pipeline, Default Audio Pipeline, and Default Image Pipeline. These pipelines are responsible for creating copies, thumbnails, and extracting technical metadata from media assets processed by the storage connectors process. When supported new media files are uploaded to connected S3 buckets, default pipelines automatically begin processing them, ensuring all your media assets are processed consistently. The figure below illustrates the default video code in the drag-and-drop interface.\nMedia lake supports importing pre-configured pipeline templates; the source code repository includes sample templates illustrating common media processing patterns. When importing pipeline templates, the system requires you to map necessary integrations while maintaining secure credential management across all your workflows.\nThis guidance system collects media asset metadata during pipeline execution to help you better understand and work more effectively with media assets. This metadata is stored via two complementary AWS services: Amazon DynamoDB serves as the primary metadata repository, while Amazon OpenSearch Service provides full-text search capabilities. When pipelines process media assets, technical metadata is extracted — including resolution, bit depth, sample rate, codec information, bitrate, and duration. Media lake automatically stores this data in DynamoDB and indexes it in OpenSearch Service.\nFor audio and video content, AI-powered transcription services generate time-aligned text segments, making spoken content searchable and accessible. You can also configure and add additional metadata fields for your organization to track, view, and use. This dual storage approach enables lightning-fast searches, clear understanding of asset quality and technical specifications, access to AI-generated insights, and file extraction in formats suitable for downstream workflows.\nAfter users search for assets, they can navigate to the asset detail screen to view their media assets. For audio and video assets, the system architecture uses Omakase Player, an open-source web player designed for media supply chain use cases. Omakase Player provides a frame-accurate viewing experience and allows displaying time-correlated metadata with the proxy.\nWhen a user selects an asset, the guidance system generates a secure, time-limited Amazon S3 pre-signed URL, allowing access to the proxy stored in Amazon S3. These pre-signed URLs allow streaming content directly from Amazon S3 and automatically expire after a defined period, ensuring convenient access while maintaining security.\nThe asset detail page provides a detailed interface for your media assets, combining preview, download options, and detailed metadata. You can quickly view the media proxy for playback while retaining access to both original files and proxy versions serving different workflow requirements.\nConclusion The guidance system for media lake on AWS illustrates how to build a media management solution on AWS, helping consolidate assets from multiple S3 buckets into a searchable catalog while creating media workflows. This system uses serverless services such as Amazon CloudFront, Amazon API Gateway, AWS Lambda, and Amazon DynamoDB to build a scalable media repository.\nKey capabilities include creating visual media workflows (called pipelines), using AWS Step Functions for automated media processing, and using AWS Secrets Manager to manage credentials securely in third-party integrations. Users can search content in two ways: through traditional metadata search, or AI-powered semantic search capable of understanding the meaning behind search phrases. This guidance helps you maintain current storage on Amazon S3 while combining centralized discovery capabilities, workflow automation, and professional media playback through frame-accurate proxy streaming.\nBy deploying Guidance for a media lake on AWS patterns, you can transform disparate media repositories into an intelligent system capable of processing, indexing, and displaying content through a single interface. Your teams will be able to efficiently search and use media assets across the entire organization, regardless of storage location.\nContact AWS Representative for more details on how we can help accelerate your business.\nFurther Reading NAB SHOW 2025 Demo Presentation – Enhancing Podcast Content AWS for Media \u0026amp; Entertainment Guidance on Event-Driven Media Workflow Automation on AWS Author Robert Raver Robert Raver is Sr. Solutions Architect for media and entertainment at AWS, leading Media Supply Chain initiatives. He designs innovative cloud architectures to optimize media workflows, addressing industry challenges related to media processing, rights, titles, QC, and metadata management. Robert has over 20 years of experience in roles related to Media \u0026amp; Entertainment and Finance industries, including leadership, architecture, and engineering.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Dynamic configuration updates in .NET using Parameter Store and Secrets Manager Loading configuration and secrets in .NET applications is a common task. However, this comes with challenges in secure storage and flexible access without requiring application restarts.\nAWS Systems Manager Parameter Store provides a centralized solution for storing and managing configuration data and sensitive information. This blog explores an advanced approach to managing them in .NET applications, focusing on:\nReferencing AWS Secrets Manager secrets through Parameter Store Loading configuration from both Parameter Store and Secrets Manager Implementing automatic configuration reloading without application restarts Solution Overview You can retrieve secrets stored in Secrets Manager directly from Parameter Store using the following format:\n/aws/reference/secretsmanager/\u0026lt;secret-path\u0026gt; By following this format, Systems Manager understands it needs to retrieve the secret from Secrets Manager instead of Parameter Store.\nFor example, to reference a secret named dev/DbPassword, you would use:\n/aws/reference/secretsmanager/dev/DbPassword See Referencing AWS Secrets Manager secrets from Parameter Store parameters for more information.\nThis solution uses Parameter Store and Secrets Manager, combined with .NET configuration and IOptionsMonitor pattern, to address the following common configuration challenges:\nUnified Access: Use Parameter Store as the single access point for both configuration and Secrets Manager secrets. Security: Sensitive data is stored in Secrets Manager and referenced in Parameter Store, ensuring proper encryption and access control. Dynamic Updates: Implement automatic reloading of configuration values without application restarts. Environment Separation: Use hierarchical structure in Parameter Store to separate configuration by environment. Prerequisites AWS account with necessary permissions .NET 8 SDK Architecture Consider an application running on an EC2 instance that needs to access both configuration parameters and sensitive secrets during operation. This application assumes an IAM role with necessary permissions and retrieves values from Parameter Store for both configuration data and secrets.\nWhen the application requests a configuration parameter, Parameter Store returns the value directly. If the parameter is a reference to a secret stored in Secrets Manager, Parameter Store retrieves the secret from Secrets Manager and returns it to the application. Hands-on Guide Let\u0026rsquo;s explore how to retrieve configuration values from AWS services in the above application. The application needs to access two types of data:\nSystem parameters like Database Hostname and API Endpoint from Parameter Store Sensitive information like Database Password and API Key from Secrets Manager In this guide, you\u0026rsquo;ll build a .NET application capable of securely accessing configuration values from both AWS services.\nStep 1: Configure parameters and secrets Store parameters in AWS Systems Manager Set up parameters in AWS Systems Manager using the following hierarchical structure:\nDevelopment Environment Production Environment /dev/DbHostname /prod/DbHostname /dev/ApiEndpoint /prod/ApiEndpoint Store secrets in AWS Secrets Manager Set up secrets in AWS Secrets Manager using the following hierarchical structure:\nDevelopment Environment Production Environment /dev/DbPassword /prod/DbPassword /dev/ApiKey /prod/ApiKey Create IAM Role to access configuration parameters and secrets To access parameters and secrets, you need to create an IAM Role with the following permissions. This role grants necessary permissions to access required resources.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ssm:GetParameter\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:GetParametersByPath\u0026#34; ], \u0026#34;Resource\u0026#34;: [\u0026#34;arn:aws:ssm:${Region}:${AccountId}:parameter/*\u0026#34;] }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;arn:aws:secretsmanager:${Region}:${AccountId}:secret:*\u0026#34;] } ] } Step 2: Set up .NET Application Configure AWS Services Extension Create two extension methods to configure AWS services as follows.\nThe following extension method adds application configuration from Parameter Store based on the provided path.\npublic static void AddParameterStoreConfiguration(this WebApplicationBuilder builder) { var configuration = builder.Configuration; // Retrieve current environment (e.g., /dev, /prod) var appSettingsPath = configuration[\u0026#34;Environment\u0026#34;]; configuration.AddSystemsManager(options =\u0026gt; { options.Path = appSettingsPath; options.ReloadAfter = TimeSpan.FromMinutes(5); // Reloads every 5 minutes }); } The following extension method adds configuration from Secrets Manager using the Systems Manager reference path.\npublic static void AddSecretsManagerConfiguration(this WebApplicationBuilder builder) { var configuration = builder.Configuration; // Retrieve current environment (e.g., /dev, /prod) var environmentPath = configuration[\u0026#34;Environment\u0026#34;]; var secretPath = $\u0026#34;/aws/reference/secretsmanager/{environmentPath}/\u0026lt;SecretName\u0026gt;\u0026#34;; configuration.AddSystemsManager(options =\u0026gt; { options.Path = secretPath; options.ReloadAfter = TimeSpan.FromMinutes(5); options.Optional = true; }); } AppSettings For local testing, please configure your appSettings.json file as follows.\n\u0026#34;AppConfig\u0026#34;: { \u0026#34;DbHostname\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ApiEndpoint\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;Secrets\u0026#34;: { \u0026#34;DbPassword\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ApiKey\u0026#34;: \u0026#34;\u0026#34; } Configuration Model A model is used to bind configuration values from Secrets Manager and Parameter Store. Add the following two classes to your project:\npublic class SecretSettings { public string DbPassword { get; set; } public string ApiKey { get; set; } } public class AppConfig { public string DbHostname { get; set; } public string ApiEndpoint { get; set; } } Startup Configuration Register AWS services configuration and necessary configuration for IOptionsMonitor during startup:\n// Add AWS Configuration. builder.AddParameterStoreConfiguration(); builder.AddSecretsManagerConfiguration(); builder.Services.Configure\u0026lt;AppConfig\u0026gt;(builder.Configuration.GetSection(\u0026#34;AppConfig\u0026#34;)); builder.Services.PostConfigure\u0026lt;SecretSettings\u0026gt;(options =\u0026gt; { options.ApiKey = builder.Configuration[\u0026#34;ApiKey\u0026#34;]; options.DbPassword = builder.Configuration[\u0026#34;DbPassword\u0026#34;]; }); Using Dynamic Configuration Use IOptionsMonitor\u0026lt;T\u0026gt; to access the latest configuration values and react to changes. This approach provides flexibility in refreshing secrets without redeployment or application restarts.\npublic class MyService { private readonly IOptionsMonitor\u0026lt;AppConfig\u0026gt; _appConfig; private readonly IOptionsMonitor\u0026lt;SecretSettings\u0026gt; _secretSettings; public MyService( IOptionsMonitor\u0026lt;AppConfig\u0026gt; appConfig, IOptionsMonitor\u0026lt;SecretSettings\u0026gt; secretSettings) { _appConfig = appConfig; _secretSettings = secretSettings; } public void DoSomething() { // Always get the latest configuration var config = _appConfig.CurrentValue; var apiEndpoint = config.ApiEndpoint; //Access parameter store value. var dbPassword = _secretSettings.CurrentValue.DbPassword; //Access secret value. } } How It Works Here\u0026rsquo;s how the application retrieves values from Secrets Manager and Parameter Store:\nWhen the application starts, it loads configuration from Parameter Store for the specified environment. The application loads both configuration and secrets (using reference path) directly from Parameter Store. AWS SDK for .NET automatically resolves these references, retrieving the actual secret values from Secrets Manager. Configuration is reloaded every 5 minutes (as set in ReloadAfter). IOptionsMonitor\u0026lt;T\u0026gt; ensures the application always uses the latest configuration values. Any changes to parameters in Parameter Store or secrets in Secrets Manager will be updated in the application within 5 minutes, without requiring a restart. Best Practices and Considerations Follow these best practices for managing configuration in .NET:\nEnvironment Separation: Use separate paths for each environment to maintain clear separation. Least Privilege: Ensure IAM roles only have the minimum necessary permissions to access Parameter Store and Secrets Manager. Encryption: Use AWS Key Management Service (AWS KMS) keys to encrypt sensitive data. Error Handling: Add robust error handling for configuration loading failure scenarios. Caching: Consider implementing a caching layer for frequently accessed but rarely changed values to reduce API calls. Audit Logging: Enable AWS CloudTrail to log parameter and secret access. Cleanup To avoid ongoing costs and maintain a clean AWS environment, perform the following steps to delete created resources:\nDelete parameters in Parameter Store: Open the AWS Systems Manager console. In the navigation panel, choose Parameter Store. On the My parameters tab, select the checkbox next to each parameter to delete. Choose Delete. In the confirmation dialog box, choose Delete parameters. Delete secrets in Secrets Manager: Open the AWS Secrets Manager console. In the secrets list, choose the secret you want to delete. In the Secret details section, choose Actions, then choose Delete secret. In the Disable secret and schedule deletion dialog box, under Waiting period, enter the number of days to wait before permanent deletion. Secrets Manager adds a field called DeletionDate and sets the value to the current date and time plus the specified number of days for the recovery window. Choose Schedule deletion. Conclusion This approach provides a robust, secure, and flexible method for managing configuration in .NET applications using AWS services. By leveraging the combination of Parameter Store and Secrets Manager, you can centrally manage configuration, securely handle sensitive information, update configuration dynamically without application restarts, and maintain clear separation between environments. This integration provides a solid solution for managing application settings and secrets in a cloud-native context, while enhancing both security and operational efficiency. To learn more, see Referencing AWS Secrets Manager secrets from Parameter Store parameters in the Systems Manager User Guide.\nAuthors Raghavender Madamshitti Raghavender Madamshitti is Lead Consultant at AWS Professional Services, bringing expertise in modernizing .NET workloads and building cloud-based solutions on AWS.\nMahesh Kumar Vemula Mahesh Kumar Vemula is Lead Consultant at AWS Professional Services. He is a serverless enthusiast, helping customers modernize .NET workloads with cloud-based solutions.\nSandeep Tammisetty Sandeep Tammisetty is Lead Consultant at AWS Professional Services. He helps customers migrate and modernize .NET workloads to AWS with cloud-based solutions.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.9-section9/5.9.1-subsection1/","title":"Clean up resources","tags":[],"description":"","content":"Clean up resources To avoid unexpected costs after completing the Workshop, delete resources in the following order:\n1. NAT Gateway: Delete NAT Gateway \u0026gt; Wait for Deleted \u0026gt; Release Elastic IP (Most important as this costs the most)\n2. Elastic Beanstalk: Terminate Environment\n3. ElastiCache: Delete Redis Cluster (Uncheck Create Backup)\n4. RDS: Stop (or Delete if no longer needed - remember to uncheck Final Snapshot).\n5. WAF: Manage resources \u0026gt; Disassociate \u0026gt; Delete protection pack (web ACL)\n6. S3: Empty and Delete Bucket (Can keep if still in use as the cost is not too high)\n7. CloudFront: Disable and Delete\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.7-section7/5.7.1-subsection1/","title":"Configure S3 &amp; CloudFront","tags":[],"description":"","content":"Configure S3 \u0026amp; CloudFront 1. Create S3 Bucket:\nCreate Bucket (e.g., minimarket-assets-prod) Block Public Access: On Manually upload the images folder from the code to this Bucket 2. Create CloudFront Distribution:\nOrigin type: Select Elastic Load Balancer Origin Domain: Select the Beanstalk Load Balancer Settings: Select Customize origin settings Protocol: HTTP Only Cache settings: Select Customize cache settings Viewer Protocol Policy: Redirect HTTP to HTTPS 3. Add S3 Origin (For images):\nGo to the newly created Distribution Go to the Origins tab \u0026gt; Create Origin Origin domain: Select the S3 bucket created earlier (minimarket-assets-prod) Origin Access: Select Origin access control (OAC) \u0026gt; Create new OAC Bucket Policy: Copy the policy provided by CloudFront and paste it into the S3 Bucket policy 4. Configure Behavior:\nReturn to CloudFront and go to the Behaviors tab Create Behavior with Path pattern: /images/ Point to Origin S3 Cache Policy: CachingOptimized "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.6-cleanup/5.6.1-section1/","title":"Create Build Project","tags":[],"description":"","content":"Create Build Project Access CodeBuild \u0026gt; Create project Project name: MiniMarket-Build Source: Select GitHub (Connect to the repository containing the code) Environment: Environment image: Managed Image Operating system: Amazon Linux Runtime: Standard Image: 5.0 Service role: New service role Privileged: Enable (Required to run Docker build commands) Buildspec: Use a buildspec file Click Create build project After creation, go to the IAM Role of the newly created CodeBuild and grant additional AmazonEC2ContainerRegistryPowerUser permission so it can push images to ECR.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"03/10/2025 AWS GenAI Builder Speaker: Toàn Huỳnh - Senior Specialist Solutions Architect\nAI-Driven Development Lifecycle (AI-DLC) Introduction to AI-DLC AI-DLC is not just a simple tool Evolution of AI in software development: 2023: Helping developers write code faster 2024: Generating larger pieces of code and answering faster 2025: Completing development tasks end-to-end with human in the loop AI should be used in a pair-programming model rather than working alone Developers are still the ones who review, validate, and own the code AI Disruption to Development Challenges when using AI:\nNeed to understand why AI is needed AI is becoming an indispensable tool in software development Software quality is still checked and ensured by developers When using AI to scale large projects, many issues can arise: Generating lots of code but unable to control quality May have to start over if not well controlled Difficult to track what AI is doing Should not \u0026ldquo;single-shot\u0026rdquo; for all cases When viewing AI as a companion to solve problems together, most problems will be solved better Working with AI-DLC Important Principles:\nDevelopers must make the final decisions, not let AI decide on its own Developers need to synthesize information, generate ideas, and communicate with AI AI will help define tasks, avoiding loss of control Must provide sufficient information and ideas to AI Development based on analyzing and understanding problems with AI support Workflow:\nImplementation Steps:\nPut the problem into AI to generate user stories Scope down and ask questions about the app you want to build Ask AI to group important units for end users Implement each unit, breaking them into small projects Mob Elaboration \u0026amp; Construction Mob Elaboration: Multiple people working together on one computer and sharing output with each other\nNote: Starting from a simple requirement through each unit to reach the final result. If Backend and Frontend want to work together according to this model, they need at least one common track to interact with each other.\nWorkflow with AI Standard Development Workflow:\nCurrently we use AI to compile code This model cannot be broken down, so for large projects, Amazon Q should be used Prompt Template:\nHow to write effective prompts:\nMust clearly specify what AI does, what Product Manager does Must indicate the role to solve the problem When wanting to do a project, ask AI to plan first Can divide into multiple options and do gradually to avoid overload Best Practices:\nStore results in Markdown files and use them for subsequent tasks Must compile multiple times, upload to MD and define clearly Put MD files into the project so AI can read and add ideas Have checkmarks in the plan file to note what has been done Important Principles Never Single-shot a Multi-step Problem:\nLeverage AI for standardized work Do thinking work yourself Must review regularly, don\u0026rsquo;t delegate tasks completely to AI Maximize Semantics per Token:\nCode is easy to over-context, so must build into Markdown files to avoid going wrong Conclusion: You are still responsible for the final code.\nKiro - The AI IDE for Prototype to Production Speaker: My Nguyễn - Senior Prototyping Architect\nIntroduction to Kiro Kiro is an AI IDE specifically designed to quickly and efficiently convert from prototype to production.\nDifferences between Kiro and VSCode:\nKey Features Power, Flexibility, and Security:\nKiro provides a powerful combination of:\nPower: Strong processing capability with AI support Flexibility: Flexible in developing from prototype to production Security: Ensuring safety and security during the development process Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Initialize VPC &amp; Subnets","tags":[],"description":"","content":"Initialize VPC \u0026amp; Subnets Open the Amazon VPC console (Note: Select the appropriate region for your needs, here we use Region ap-southeast-1)\nChoose Create VPC\nConfiguration: Name: MiniMarket-VPC IPv4 CIDR: 10.0.0.0/16 Create Subnets (Divide into 2 AZs to ensure High Availability): Public Subnets (2): 10.0.1.0/24 \u0026amp; 10.0.2.0/24 (Used for Load Balancer \u0026amp; NAT) Private Subnets (2): 10.0.3.0/24 \u0026amp; 10.0.4.0/24 (Used for App, DB, Redis) Click Create VPC and wait for the state to change to Available to complete successfully "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Huu Vinh\nPhone Number: 0948030249\nEmail: huuvinh05926@gmail.com\nUniversity: FPTU HCMC\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Introduction to Human Resource Management (HRM) System The HRM system is a core platform developed on Java Spring Boot, designed to comprehensively manage employee records, attendance processes, and other HR tasks. This workshop focuses on migrating the application from an On-premise environment to a Cloud-Native architecture on AWS, with particular emphasis on ensuring sensitive data security and scalability.\nThis migration strictly adheres to the pillars of the AWS Well-Architected Framework: Security, Reliability, Performance Efficiency, and Cost Optimization.\nSolution Architecture Overview on AWS The HRM project utilizes a Microservices architecture and carefully selected AWS services to maximize security for sensitive HR data.\nSolution Architecture:\nCompute \u0026amp; Logic Layer: AWS Elastic Beanstalk (or ECS): Leverages the Docker platform to package and deploy the Java Spring Boot application. This simplifies infrastructure management, automates Auto Scaling, and provides Elastic Load Balancing. AWS Lambda \u0026amp; API Gateway: Used to handle asynchronous operations (e.g., report generation, salary calculations) or lightweight APIs, optimizing cost and performance. Data \u0026amp; Persistence Layer: Amazon RDS for PostgreSQL: Serves as the primary relational database (RDB) for storing employee records, salary information, and sensitive data. Placed in a Private Subnet to maintain the highest level of security. Amazon DynamoDB: Used for unstructured, high-speed data (e.g., storing record change history, transaction logs). Amazon S3: Used to store and distribute static resources such as profile pictures and face check-in images with superior durability. Security \u0026amp; Authentication: Amazon Cognito: Provides user identity management services, ensuring authentication and access authorization for employees and managers. AWS Secrets Manager: Integrated to securely store and manage API keys, database credentials, and Tokens for session creation or external integrations. AWS WAF \u0026amp; Amazon CloudFront: Provides application protection against web threats (OWASP Top 10) and secure global content distribution. DevOps \u0026amp; Monitoring: AWS CodePipeline \u0026amp; CodeBuild: Automates the CI/CD process from source code repository (GitHub) to AWS environment, ensuring fast and reliable deployment. Amazon CloudWatch \u0026amp; AWS SNS: Monitors system metrics (CPU, Network, Latency) and configures real-time alert notifications (via SNS) to ensure service reliability. "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.8-section8/5.8.1-subsection1/","title":"Monitor with CloudWatch","tags":[],"description":"","content":"Monitor with CloudWatch 1. Create SNS Topic:\nGo to SNS \u0026gt; Topics \u0026gt; Create Topic Type: Standard Name: DevOps-Alerts 2. Create Subscription:\nCreate Subscription \u0026gt; Protocol: Email \u0026gt; Enter your email (Remember to Confirm email) 3. Create CPU Alarm:\nGo to CloudWatch \u0026gt; Alarms \u0026gt; Create alarm Select metric \u0026gt; EC2 \u0026gt; Per-Instance Metrics \u0026gt; Select the InstanceID of Beanstalk \u0026gt; CPUUtilization Condition: CPUUtilization: Greater than 70% Notification: Select Topic DevOps-Alerts Create Alarm. "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.5-policy/5.5.1-section1/","title":"Package with Docker","tags":[],"description":"","content":"Package with Docker Before deploying to the Cloud, we need to package the .NET Core application into a Docker Image.\n1. Create Dockerfile In the root directory of the Solution, create a file named Dockerfile (without extension)\n# STAGE 1: BUILD FROM maven:3.9-eclipse-temurin-21 AS build WORKDIR /app COPY pom.xml . RUN mvn dependency:go-offline -B COPY src ./src RUN mvn clean package -DskipTests # STAGE 2: RUNTIME FROM eclipse-temurin:21-jre-alpine WORKDIR /app # Create non-root user to run application (security best practice) RUN addgroup -S spring \u0026amp;\u0026amp; adduser -S spring -G spring USER spring:spring # Copy JAR from build stage COPY --from=build /app/target/*.jar app.jar # Expose port EXPOSE 8085 # Health check (comment out if Spring Actuator is not available) # HEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3 \\ # CMD wget --no-verbose --tries=1 --spider http://localhost:8085/actuator/health || exit 1 # Run application ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;app.jar\u0026#34;] 2. Create buildspec.yml Create buildspec.yml file to instruct AWS CodeBuild how to package and push to ECR\nversion: 0.2 phases: pre_build: commands: - echo Logging in to Amazon ECR... # --- CONFIGURATION INFORMATION --- - AWS_DEFAULT_REGION=ap-southeast-1 # Replace with your Account ID in the line below: - AWS_ACCOUNT_ID=YOUR ACCOUNT ID - IMAGE_REPO_NAME=market-app - IMAGE_TAG=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7) - REPOSITORY_URI=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME # --------------------------- - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com build: commands: - echo Build started on `date` - echo Building the Docker image... - docker build -t $REPOSITORY_URI:latest . - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG post_build: commands: - echo Build completed on `date` - echo Pushing the Docker image... - docker push $REPOSITORY_URI:latest - docker push $REPOSITORY_URI:$IMAGE_TAG - echo Writing image definitions file... # Automatically generate Dockerrun.aws.json configuration file for Beanstalk # Map Port 80 (Host) to 8080 (Container .NET) - printf \u0026#39;{\u0026#34;AWSEBDockerrunVersion\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;Image\u0026#34;:{\u0026#34;Name\u0026#34;:\u0026#34;%s\u0026#34;,\u0026#34;Update\u0026#34;:\u0026#34;true\u0026#34;},\u0026#34;Ports\u0026#34;:[{\u0026#34;ContainerPort\u0026#34;:8080,\u0026#34;HostPort\u0026#34;:80}]}\u0026#39; \u0026#34;$REPOSITORY_URI:$IMAGE_TAG\u0026#34; \u0026gt; Dockerrun.aws.json - cat Dockerrun.aws.json artifacts: files: - Dockerrun.aws.json "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Setup Security Groups","tags":[],"description":"","content":"Setup Security Groups Access EC2 \u0026gt; Security Groups \u0026gt; Create security group Group 1: Web Server (sg-web-app) Description: Allow HTTP from the Internet Inbound Rules: Type: HTTP (80) | Source: 0.0.0.0/0 (Or only from Load Balancer for stricter security) Group 2: Database (sg-db-sql) Description: Only allow access from Web Server Inbound Rules: Type: MSSQL (1433) | Source: Custom \u0026gt; Select sg-web-app ID Group 3: Redis Cache (sg-redis-cache) Description: Only allow access from Web Server Inbound Rules: Type: Custom TCP (6379) | Source: Custom \u0026gt; Select sg-web-app ID Click Save changes "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey (FCJ). Understand basic AWS services, how to use AWS Management Console and AWS CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members. - Read and understand the rules and regulations at the internship unit. 08/11/2025 08/11/2025 3 - Set up \u0026amp; manage AWS via Console and CLI. 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account. - Learn about AWS Console \u0026amp; AWS CLI. - Practice: + Create AWS account. + Install AWS CLI \u0026amp; configure. + Use AWS CLI basics. 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types, AMI, EBS,\u0026hellip; - Methods to remote SSH into EC2. - Learn about Elastic IP. 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch EC2 instance. + Connect via SSH. + Attach EBS volume. 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Learned how to draw AWS architecture on draw.io and use standard AWS Architecture Icons.\nSuccessfully created and configured AWS Free Tier account:\nUnderstood the concept of Root Account and IAM User. Created IAM Group and IAM User, assigned admin permissions. Distinguished Access Key/Secret Key (CLI) and Console Password (Web login). Became familiar with AWS Management Console:\nKnew how to log in and navigate the web interface. Managed User Groups, Users, and Permissions in IAM. Understood the significance of setting passwords for users. Installed and configured AWS CLI v2 on computer:\nSet up Access Key, Secret Key, default Region. Connected CLI to AWS account using command: aws sts get-caller-identity Used AWS CLI to perform basic operations:\nChecked account information (aws sts get-caller-identity). Retrieved list of regions (aws ec2 describe-regions). Viewed EC2 information (aws ec2 describe-instances). Created and managed key pairs (aws ec2 create-key-pair). Uploaded files to S3 (aws s3 cp \u0026lt;file\u0026gt; s3://\u0026lt;bucket\u0026gt;). Acquired the ability to combine Console and CLI to manage AWS resources in parallel.\nAdditional learning:\nHow to create domain via Route 53. Introduction to CDN, AWS WAF, and other security services. Managing spending on AWS Billing Dashboard. Learning about AWS Support Plans. "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand thoroughly the Amazon VPC architecture and how network components work in AWS. Master the models of Subnet, Route Table, Security Group, NACL, VPC Endpoint, Peering, and Load Balancer. Know how to choose appropriate connectivity solutions between on-premises environments and AWS Cloud. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Amazon VPC concept \u0026amp; structure, account limits, IPv4/IPv6 CIDR. - Configure basic VPC via Console. 08/18/2025 08/18/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Subnet, Availability Zone, IP addresses. - Create public/private subnets and assign corresponding route tables. 08/19/2025 08/19/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Route Table, Internet Gateway, NAT Gateway, VPC Endpoint. - Distinguish interface \u0026amp; gateway endpoints. 08/20/2025 08/20/2025 https://cloudjourney.awsstudygroup.com/ 5 - Event: Attended Vietnam Cloud Day 2025 – Track 1: GenAI and Data. - Overview of AI applications in cloud infrastructure. 08/21/2025 08/21/2025 6 - Study Security Group, Network ACL, VPC Flow Logs. - Practice logging \u0026amp; controlling access between subnets. 08/22/2025 08/22/2025 https://cloudjourney.awsstudygroup.com/ 7 - Learn about VPC Peering, Transit Gateway, VPN, Direct Connect, Elastic Load Balancer (ALB, NLB, CLB, GLB). 08/23/2025 08/23/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understood clearly that VPC (Virtual Private Cloud) is a virtual network environment in AWS that helps separate and manage resources by environment (Production/Dev/Test/Staging).\nDefault limit: 5 VPCs per AWS account. Each VPC requires an IPv4 CIDR block (mandatory) and can add IPv6 (optional). Subnet:\nEach subnet resides in a specific Availability Zone. When creating a subnet, must specify a CIDR within the VPC\u0026rsquo;s CIDR range. AWS reserves the first 5 IP addresses in each subnet for internal use. Route Table:\nDefines network routing. Has a default default route table (cannot be deleted) that allows internal communication within the VPC. Elastic Network Interface (ENI) \u0026amp; Elastic IP:\nENI is a virtual network card that can be assigned to EC2 and moved between instances. Elastic IP is a static public IPv4 address that can be attached to ENI. VPC Endpoint:\nConnects resources in VPC to AWS services without needing Internet. Two types: Interface Endpoint – uses ENI + private IP. Gateway Endpoint – uses route table (supports S3 and DynamoDB). Internet Gateway \u0026amp; NAT Gateway:\nInternet Gateway: allows EC2 in public subnet to access Internet, managed by AWS, no need for autoscale configuration. NAT Gateway: allows private subnet to access Internet (outbound only). Security Group (SG) and Network ACL (NACL):\nSG is a stateful firewall applied to ENI, only has \u0026ldquo;allow\u0026rdquo; rules. NACL is a stateless firewall applied to subnet, has \u0026ldquo;allow/deny\u0026rdquo; rules and reads from top to bottom. Default: SG blocks inbound, allows outbound; NACL allows all. VPC Flow Logs:\nRecords IP traffic to/from VPC, subnet, or ENI. Stores on CloudWatch Logs or S3, does not record packet contents. VPC Peering \u0026amp; Transit Gateway:\nVPC Peering: direct connection between 2 VPCs (1:1), does not support transitive routing, CIDRs must not overlap. Transit Gateway: hub center connecting multiple VPCs or on-premises networks. VPN Site-to-Site \u0026amp; Client-to-Site:\nSite-to-Site: connects data center to AWS via Virtual Private Gateway and Customer Gateway. Client-to-Site: allows 1 host to access resources in VPC (VPN client). AWS Direct Connect:\nCreates physical connection from data center to AWS (via Direct Connect Partner in Vietnam). Low latency (~20–30ms), adjustable bandwidth. Elastic Load Balancing (ELB):\nDistributes traffic to multiple EC2/Containers. Has 4 types: Application Load Balancer (ALB) – Layer 7, supports path-based routing. Network Load Balancer (NLB) – Layer 4, high performance, supports static IP. Classic Load Balancer (CLB) – Layer 4 \u0026amp; 7, old, rarely used. Gateway Load Balancer (GLB) – Layer 3, uses GENEVE protocol (port 6081). Supports Sticky Session and Access Logs (stored in S3). Attended Vietnam Cloud Day 2025 - Track 1: GenAI and Data event which helped understand better how AWS combines artificial intelligence and big data to optimize infrastructure, accelerate data analysis, and enhance customer experience in cloud-native systems.\nView EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel. \u0026hellip; "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn in-depth about Amazon EC2 and related storage services (EBS, EFS, FSx). Understand the operating mechanisms, configuration, and pricing options of EC2. Practice creating and managing EC2 Instances, EBS Volumes, Snapshots. Understand and apply Auto Scaling, Pricing Options, and Lightsail. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of Amazon EC2: concept, scalability, comparison with physical servers.\n- Learn about Instance Types, technical specifications (CPU, Memory, Network, Storage). 08/25/2025 08/25/2025 https://cloudjourney.awsstudygroup.com/ 3 - Study AMI (Amazon Machine Image), EC2 Instance provisioning process.\n- Learn about Hypervisor (KVM, HVM, PV) and selection mechanisms. 08/26/2025 08/26/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Key Pair and login information encryption mechanism (Linux/Windows).\n- Learn about EBS (Elastic Block Store): HDD types, SSD, snapshot, data replication. 08/27/2025 08/27/2025 https://cloudjourney.awsstudygroup.com/ 5 - Study Instance Store: characteristics, pros/cons, practical applications.\n- Practice backing up EBS using snapshots and recovery. 08/28/2025 08/28/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about User Data and Metadata in EC2, how to automate during Instance initialization.\n- Learn about EC2 Auto Scaling, Scaling Policy, Load Balancer integration. 08/29/2025 08/29/2025 https://cloudjourney.awsstudygroup.com/ 7 - Learn about Pricing Options: On-demand, Reserved Instance, Saving Plan, Spot Instance.\n- Study Amazon Lightsail, EFS, FSx, and AWS Application Migration Service (MGN). 08/30/2025 08/30/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Understood clearly the architecture and operation of Amazon EC2:\nSimilar to traditional virtual machines but with flexible scalability. Can launch quickly, supports many types of workloads like web, applications, databases\u0026hellip; Mastered the mechanisms of EC2 Instance Type, AMI, Hypervisor, Key Pair.\nKnew how to create, connect, and backup EC2 Instances through EBS snapshots.\nClearly understood the differences between EBS, Instance Store, EFS, and FSx:\nEBS: block storage attached directly to EC2, operates independently via private network. Instance Store: extremely high speed, does not retain data when stopping instance. EFS: shared storage for multiple EC2 (Linux). FSx: similar to EFS but supports NTFS and SMB (Windows/Linux). Mastered EC2 Auto Scaling:\nAutomatically increases/decreases number of Instances. Supports multiple AZs and integrates with Load Balancer. Can combine multiple Pricing Options. Clearly understood the 4 main Pricing Options:\nOn-demand: flexible, high price. Reserved Instance \u0026amp; Saving Plan: save when committing long-term. Spot Instance: cheap price, but can be stopped suddenly. Knew how to use Amazon Lightsail for light workloads, test/dev.\nUnderstood the mechanism of AWS Application Migration Service (MGN) to copy physical/virtual servers to EC2.\nClearly understood the process of data replication, incremental snapshots, and cost optimization through deduplication.\nCompleted comprehensive documentation and detailed notes about the entire chain of services related to EC2.\nView EC2 service\nCreate and manage key pairs\nCheck information about running services\n\u0026hellip;\nAcquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn in-depth about Amazon Simple Storage Service (S3) and related storage services. Understand object-based storage structure, distinguish from block storage. Master storage classes, lifecycle policies, CORS, and versioning mechanisms. Learn about Amazon Glacier, Snow Family, AWS Storage Gateway, and AWS Backup. Understand RTO/RPO concepts and Disaster Recovery strategies. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Amazon S3: concept, characteristics of object-based storage.\n- Learn about data replication mechanism, 99.999999% durability and 99.99% availability. 09/01/2025 09/01/2025 https://cloudjourney.awsstudygroup.com/ 3 - Study bucket, object, key, access point.\n- Learn about storage classes: Standard, IA, Intelligent Tiering, One Zone, Glacier, Deep Archive. 09/02/2025 09/02/2025 https://cloudjourney.awsstudygroup.com/ 4 - Configure S3 lifecycle policy to automatically transition storage tiers.\n- Learn about multipart upload and event triggers when uploading/deleting files. 09/03/2025 09/03/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about S3 hosting static websites, CORS policy, Access Control List (ACL), Bucket Policy, IAM Policy.\n- Practice setting up access permissions. 09/04/2025 09/04/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about S3 Versioning, S3 Endpoint, partition \u0026amp; prefix performance optimization.\n- Study Amazon Glacier: data retrieval mechanism (Expedited, Standard, Bulk). 09/05/2025 09/05/2025 https://cloudjourney.awsstudygroup.com/ 7 - Study Snow Family (Snowball, Snowball Edge, Snowmobile).\n- Learn about AWS Storage Gateway (File, Volume, Tape).\n- Learn about AWS Backup, RTO/RPO, and Disaster Recovery Strategies. 09/06/2025 09/06/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Mastered knowledge about Amazon S3:\nIs an object storage service, cannot edit parts but must upload entirely. Data replicated across 3 Availability Zones in the same region. Supports trigger events, multipart upload, versioning, and CORS configuration. Clearly understood storage classes (Storage Class):\nS3 Standard: frequently accessed data. S3 Standard-IA / One Zone-IA: infrequently accessed data, lower cost. S3 Intelligent-Tiering: automatically moves data between tiers. S3 Glacier / Deep Archive: long-term storage, slow retrieval, cheap price. Practiced configuring Lifecycle Policy:\nSet up automatic object movement after X days between storage classes. Automatically delete old objects after specified time. Learned about CORS and permission mechanisms:\nCORS (Cross-Origin Resource Sharing) allows web apps to access resources from different domains. S3 ACL: basic access permissions by bucket/object. Bucket Policy / IAM Policy: define detailed access permissions using JSON policy. Understood clearly about Versioning:\nWhen versioning is enabled, deleting or overwriting does not lose old data. Supports recovering files that were deleted or overwritten incorrectly. Optimized S3 performance:\nUse random prefix to increase search performance in partitions. Understood partition \u0026amp; key map hash mechanism of S3. Amazon Glacier:\nServes long-term data storage with 3 retrieval levels: Expedited (1–5 minutes) Standard (3–5 hours) Bulk (5–12 hours) Snow Family:\nSnowball / Snowball Edge / Snowmobile used to migrate large-scale data (TB → EB) from on-premise to AWS. Can process, encrypt, and compress data before importing to S3 or Glacier. AWS Storage Gateway:\nCombines on-premise + cloud storage, includes 3 types: File Gateway (NFS/SMB) → writes to S3. Volume Gateway (iSCSI) → stores block data, snapshots to EBS. Tape Gateway (VTL) → stores virtual tapes on S3/Glacier. AWS Backup \u0026amp; Disaster Recovery:\nUnderstood concepts of RTO (Recovery Time Objective) and RPO (Recovery Point Objective). Distinguished 4 DR strategies: Backup \u0026amp; Restore Pilot Light Low Capacity Active-Active Full Capacity Active-Active AWS Backup supports EBS, EC2, RDS, DynamoDB, EFS, Storage Gateway. "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Understand clearly the Shared Responsibility Model and security responsibilities between AWS and customers. Master the Identity and Access Management (IAM) mechanism — users, groups, roles, policies. Learn about AWS Organization, AWS Identity Center, and AWS KMS in security management. Know how to assign permissions, encrypt, and audit security in AWS environment. Get familiar with Amazon Cognito and AWS Security Hub. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Learn about Shared Responsibility Model: division of responsibilities between AWS and customers; service classification by management level (Infrastructure, Hybrid, Fully-managed). 09/08/2025 09/08/2025 https://cloudjourney.awsstudygroup.com/ 3 Study Root Account and best practices: create IAM admin user, lock root credentials, protect domain/email information. 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 Learn about IAM Users, Groups, Roles, Policies — permission mechanism, trust policy, explicit deny. 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 Practice creating IAM Role, assume role, and use AWS STS (Security Token Service) to grant temporary permissions. 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 Learn about Amazon Cognito (User Pool \u0026amp; Identity Pool) — login mechanism, authentication, and access to AWS resources. 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ 7 Study AWS Organization, Identity Center, KMS, and Security Hub — centralized management, data encryption, automatic security assessment. 09/13/2025 09/13/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Shared Responsibility Model AWS is responsible for security of the cloud — hardware, infrastructure, hypervisor. Customers are responsible for security in the cloud — service configuration, data, encryption, user management. Level of responsibility changes depending on service type: Infrastructure (EC2, VPC, EBS) → customer bears most. Managed Services (RDS, EKS) → shared. Fully Managed (S3, Lambda) → AWS bears more. AWS Root Account Has full access to all AWS services and resources. Should lock credentials, only use in emergencies. Best practices: Create IAM Administrator user to replace root. Divide root access among 2 different managers. Ensure domain and root account email renewal regularly. AWS Identity and Access Management (IAM) Is a service to control access to AWS resources. Principals include: AWS Root user, IAM user, Federated user (via SAML/OAuth), IAM role, AWS service, Anonymous user. IAM User: Has no default permissions. Can log in via Management Console or use Access Key/Secret Key. Not used to manage applications/operating systems. IAM Group: Groups users for easier management (cannot nest groups). IAM Policy (JSON): Identity-based policy (attached to Principal). Resource-based policy (attached to Resource). Always prioritizes explicit Deny \u0026gt; Allow. IAM Role: Has no fixed credentials, only used when assumed. Trust policy determines who can assume role. Used for cross-account access or granting permissions to AWS services (like EC2). Associated with AWS STS to grant temporary credentials. Amazon Cognito Service for user authentication and management for web/mobile apps. Consists of 2 components: User Pool – stores, registers, and authenticates users. Identity Pool – grants access to AWS services. Supports login via username/password or third-party (Facebook, Google, Amazon). User Pool combines with Identity Pool to directly access AWS resources after authentication. AWS Organization Allows managing multiple AWS accounts within the same organization. Supports OU (Organizational Units) and Consolidated Billing. Can assign Service Control Policy (SCP) to set maximum permission limits on OU/account. SCP is deny-based, does not grant permissions but only sets boundaries. AWS Identity Center (formerly SSO) Manages access for multiple AWS accounts and external applications. Identity source can be: AWS Managed AD On-premises AD (via Trust/Connector) Permission Set: set of permissions assigned to user/group → grants role to accounts in Organization. AWS Key Management Service (KMS) Service to create and manage encryption keys for data encryption. Supports Encryption at Rest meeting FIPS 140-2 standards. CMK (Customer Managed Key) commonly used to create Data Keys for actual data encryption. AWS never exports CMK outside the system. AWS Security Hub Service for automatic security auditing based on AWS Best Practices and industry standards. Runs continuously, evaluates service configurations, and displays security score. Supports aggregating security alerts from multiple other services (GuardDuty, Config, Inspector\u0026hellip;). View EC2 service\nCreate and manage key pairs\nCheck information about running services\n\u0026hellip;\nAcquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand basic concepts of Database Concepts and RDBMS / NoSQL. Distinguish OLTP, OLAP, and corresponding data systems. Master the structure and roles of Primary Key, Foreign Key, Index, Partition, Buffer, Log. Learn about database services on AWS such as Amazon RDS, Aurora, Redshift, ElastiCache. Know how to optimize, recover, scale, and secure databases on AWS platform. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Review database concepts: Database, Session, Primary/Foreign Key, Index, Partition, Buffer, Execution Plan, DB Log. 09/15/2025 09/15/2025 https://cloudjourney.awsstudygroup.com/ 3 Learn about RDBMS (relational model, SQL) and NoSQL (Document, Key-Value, Graph, Wide-column). 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 4 Compare OLTP and OLAP, determine suitable application types and the role of Data Warehouse. 09/17/2025 09/17/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn about Amazon RDS: architecture, features (backup, read replica, failover, scaling, encryption). 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com/ 6 Learn about Amazon Aurora: read/write performance, backtrack, clone, global DB, multi-master. 09/19/2025 09/19/2025 https://cloudjourney.awsstudygroup.com/ 7 Learn about Amazon Redshift and ElastiCache, applications for OLAP and caching. 09/20/2025 09/20/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: 🧩 Database Concept Database (CSDL): structured/semi-structured information system stored on devices to serve concurrent access from multiple applications. Session: time period from connection to disconnection with database system. Primary Key: uniquely identifies each record in a table. Foreign Key: links between tables through reference to primary key. Index: data structure that helps speed up access, but consumes memory and write cost. Partition: divides large tables into multiple parts for faster queries. Execution Plan: query execution plan created by query optimizer to choose the most efficient method. DB Log: records changes to help recover and synchronize between primary–replica. Buffer: temporary memory area to speed up read/write data before synchronizing with disk. RDBMS \u0026amp; NoSQL RDBMS (Relational Database Management System):\nData organized in tables, rows, columns; relationships between tables represented by keys. Uses SQL to query and manage data. Ensures data integrity, supports ACID (Atomicity, Consistency, Isolation, Durability). NoSQL (Not only SQL):\nDoes not store data in tables, has flexible structure. Popular types: Document-based: MongoDB. Key-Value: Redis, DynamoDB. Wide-column: Cassandra. Graph: Neo4j. Suitable for big data applications, unstructured, flexible scaling. OLTP vs OLAP Feature OLTP OLAP Purpose Process real-time transactions Analyze historical data Data Frequently updated Aggregated, read-only Applications Banking, retail, ticketing Reports, BI, analysis Focus Transaction processing speed Query response speed AWS Technology RDS, Aurora Redshift, Athena, QuickSight Amazon RDS (Relational Database Service) Fully managed database service. Supports: Aurora, MySQL, PostgreSQL, MariaDB, Oracle, MSSQL. Key features: Automatic backup (DB + Log, kept up to 35 days). Read Replica supports read workload (reporting). Automatic failover between Primary/Standby (Multi-AZ). Encryption at rest \u0026amp; in transit. Auto scaling storage \u0026amp; instance size. Security via Security Group and NACL. Commonly used for OLTP applications. Amazon Aurora RDBMS optimized for high performance, compatible with MySQL and PostgreSQL. Inherits all RDS features, adds characteristics: Backtrack – restores DB to previous point in time. Clone – creates fast copy. Global Database – multiple regions with 1 master, many read replicas. Multi-master – supports parallel writes from multiple nodes. Data stored distributed and automatically synchronized across multiple AZs. Amazon Redshift Data Warehouse service managed by AWS, core is PostgreSQL optimized for OLAP. Uses Massively Parallel Processing (MPP) architecture: Leader Node coordinates queries. Compute Node stores and processes data. Stores data in columns (Columnar Storage) → optimized for analysis. Supports: SQL, JDBC, ODBC. Redshift Spectrum – queries data directly in S3. Transient cluster – saves cost when paused. Amazon ElastiCache Fully managed caching engine service. Supports two engines: Redis and Memcached. Helps reduce database load, increases data access performance. AWS automatically detects and replaces failed nodes. Redis usually recommended more because of: Rich features (replication, persistence, pub/sub). High performance and good scalability. Need to manage caching logic in application to ensure data consistency. View EC2 service\nCreate and manage key pairs\nCheck information about running services\n\u0026hellip;\nAcquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Understand AWS fundamentals and IAM concepts Analyze the project architecture of the HR System Set up S3 and configure Static Website Hosting Tasks to be carried out this week: Day Learning / Research Practice \u0026amp; Code AWS Event Attended 2 - Learn about AWS and IAM - IAM concepts: User, Group, Role, Policy - Principle of Least Privilege - Set up IAM with Least Privilege - Create IAM Users and Groups Getting Started with AWS IAM and Cloud Security 3 - Familiarize with AWS Console interface - Learn AWS CLI commands - Install and configure AWS CLI - Practice basic CLI commands 4 - Analyze project architecture - Learn about Layered Architecture - Document HR System architecture 5 - Learn about Spring Boot project structure - Maven configuration - Initialize Spring Boot project - Configure pom.xml 6 - Learn about S3 and bucket policies - Create S3 Bucket - Configure access policies 7 - Learn about Static Website Hosting - Configure Static Website Hosting on S3 AWSome Day – Introduction to AWS Services Week 7 Achievements: Set up IAM account with Least Privilege principle, understood basic concepts of User, Group, Role, Policy.\nInstalled and practiced basic AWS CLI commands.\nAnalyzed and documented the HR System architecture, clearly identifying layers: Presentation, Business Logic, Data Access, Database.\nInitialized Spring Boot project, configured pom.xml file with necessary dependencies: Spring Web, Spring Data JPA, PostgreSQL/MySQL.\nCreated S3 Bucket, configured policies to allow public read access to objects.\nSuccessfully configured Static Website Hosting on S3, uploaded index.html and verified the application runs correctly via the S3 endpoint.\nAttended AWS online events: \u0026ldquo;Getting Started with AWS IAM and Cloud Security\u0026rdquo; and \u0026ldquo;AWSome Day – Introduction to AWS Services\u0026rdquo;.\nView EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Understand CloudFront CDN and its benefits Set up Route 53 to manage domain names Learn and configure AWS WAF to protect web applications Tasks to be carried out this week: Day Learning / Research Practice \u0026amp; Code AWS Event Attended 2 - Learn CloudFront CDN concepts - CloudFront Distribution types - Origin concept (S3, HTTP/HTTPS) - Create CloudFront Distribution - Configure Origin pointing to S3 Bucket Optimizing Performance with Amazon CloudFront 3 - Learn about Cache Policy - Origin Access Control (OAC) - Configure OAC for S3 - Integrate S3 with CloudFront 4 - TTL (Time to Live) concepts - CORS concepts - Test with TTL=300 - Configure CORS headers 5 - Learn about Route 53 and DNS - Hosted Zone concepts - Create Hosted Zone - Update Nameserver with domain provider 6 - Learn about DNS record types - A Record, CNAME concepts - Create A record pointing to CloudFront 7 - Learn about AWS WAF and OWASP Top 10 - Rate Limit concepts - Create WebACL - Configure basic rules - Attach WebACL to CloudFront AWS Networking Fundamentals Week 8 Achievements: Created CloudFront Distribution, successfully integrated with S3 Bucket via OAC (Origin Access Control).\nConfigured Cache Policy and TTL=300, verified content cached at CloudFront Edge Location.\nSuccessfully configured CORS to allow cross-origin requests from JavaScript.\nCreated Hosted Zone on Route 53, updated Nameserver with the domain provider (e.g., GoDaddy).\nCreated A record pointing to CloudFront Distribution, verified domain successfully resolves to the CloudFront endpoint.\nCreated WebACL on AWS WAF, configured basic rules against OWASP Top 10 (e.g., SQL Injection, XSS), and Rate Limit (e.g., 2000 requests/5 minutes).\nSuccessfully integrated WebACL with CloudFront Distribution, verified requests are checked and blocked when violating rules.\nAttended AWS online events: \u0026ldquo;Optimizing Performance with Amazon CloudFront\u0026rdquo; and \u0026ldquo;AWS Networking Fundamentals\u0026rdquo;.\nView EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Understand API Gateway types (REST API vs HTTP API) Set up Cognito User Pool for user management Integrate JWT authentication from Cognito into API Gateway Tasks to be carried out this week: Day Learning / Research Practice \u0026amp; Code AWS Event Attended 2 - Learn about API Gateway - REST API vs HTTP API - Routes and Integration concepts - Create HTTP API - Configure basic settings Modern API Development with Amazon API Gateway 3 - Learn routing structure - HTTP Methods: GET, POST, PUT, DELETE - Create routes for /api/v1/employees 4 - Learn about complex routing - Create routes for /api/v1/attendance - Create routes for /api/v1/payroll 5 - Learn about Cognito User Pool - User Pool vs Identity Pool - App Client concepts - Create Cognito User Pool - Configure App Client 6 - Learn about JWT Token (id_token, access_token) - JWT authentication flows - Test login and retrieve JWT from Cognito 7 - Learn about Authorizers - Cognito Authorizer configuration - Configure Cognito Authorizer in API Gateway - Test with Postman Secure Your Applications with Amazon Cognito Week 9 Achievements: Created HTTP API on API Gateway, clearly understood the differences between REST API and HTTP API (cost, latency, features).\nSuccessfully configured routes for the HR System:\n/api/v1/employees (GET, POST, PUT, DELETE) /api/v1/attendance (GET, POST, PUT) /api/v1/payroll (GET) Created Cognito User Pool, configured App Client with appropriate settings (e.g., Allow username/password, Enable refresh token).\nSuccessfully tested login flow, retrieved JWT Token (id_token, access_token) from Cognito.\nConfigured Cognito Authorizer in API Gateway, integrated JWT authentication.\nUsed Postman to test APIs with JWT Token in the Authorization header, verified that requests without a token are rejected with HTTP 401.\nAttended AWS online events: \u0026ldquo;Modern API Development with Amazon API Gateway\u0026rdquo; and \u0026ldquo;Secure Your Applications with Amazon Cognito\u0026rdquo;.\nView EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS, setting up Console/CLI and learning EC2 basics\nWeek 2: Learning Amazon VPC architecture, Subnet, Route Table, Security Group and Load Balancer\nWeek 3: Researching Amazon EC2, EBS, Instance Store, Auto Scaling and Pricing Options\nWeek 4: Learning Amazon S3, Storage Class, Lifecycle Policy, Glacier and Disaster Recovery\nWeek 5: Researching IAM, Shared Responsibility Model, Cognito and AWS Security Services\nWeek 6: Learning Database Concept, RDBMS/NoSQL, RDS, Aurora, Redshift and ElastiCache\nWeek 7: Setting up HR System project: AWS IAM, Spring Boot, S3 Static Website Hosting\nWeek 8: Deploying CloudFront CDN, Route 53 DNS and AWS WAF for security\nWeek 9: Configuring API Gateway, Cognito User Pool and JWT Authorizer authentication\nWeek 10: Developing Employee and Payroll APIs with Spring Data JPA and Repository\nWeek 11: Implementing Attendance API, Lambda Flow and Leave Management System\nWeek 12: Optimizing with Redis Cache, Secrets Manager and completing API documentation\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"15/11/2025 Generative AI with Amazon Bedrock Speaker Information Lam Tuan Kiet - Sr. DevOps Engineer, FPT Software Danh Hoang Hieu Nghi - AI Engineer, Renova Cloud Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud Journey Context and Transformation Motivation Currently, many enterprises are facing outdated technology platforms. Investing in building a new platform from scratch often requires very high costs in terms of both finances and time. The optimal solution is to leverage cloud technology and serverless services to minimize operational costs and accelerate deployment speed.\nFoundation Models - The Foundation of Gen AI What is Generative AI? Generative AI (Gen AI) is a technology that allows the creation of new content such as text, images, and audio based on input prompts. The special feature of Gen AI is its ability to create content that is creative and contextually appropriate.\nFoundation Models Foundation Models are AI models trained on massive amounts of data and have the ability to:\nHandle diverse different tasks Generalize knowledge from training data Can be fine-tuned for specific applications Save time and development costs Amazon Bedrock Amazon Bedrock is a fully managed service that provides access to many leading Foundation Models through a single API.\nSupported Foundation Models Amazon Bedrock supports many models from leading providers:\nClaude (Anthropic): Strong in reasoning and analysis Titan (Amazon): Models optimized for AWS use cases Jurassic (AI21 Labs): Good for language processing Llama (Meta): Open-source, flexible customization Stable Diffusion: Specialized in image generation Prompt Engineering What is a Prompt? A prompt is a command or text that we provide to the AI model to receive the desired result. The quality of the prompt directly affects the quality of the output.\nPrompt Engineering Prompt Engineering is the art and science of designing effective prompts to optimize results from AI models. This is an important skill when working with Generative AI.\nPrompting Techniques 1. Zero-Shot Prompting Zero-Shot Prompting is a technique of asking questions or making requests directly without providing specific examples. The model will use its trained knowledge to answer.\nAdvantages:\nSimple, fast Suitable for basic questions Disadvantages:\nResults may not be accurate for complex tasks 2. Few-Shot Prompting Few-Shot Prompting provides a few illustrative examples before making the main request. This helps the model better understand the desired format and context.\nAdvantages:\nSignificantly improves accuracy Model understands requirements better Suitable for specific tasks Disadvantages:\nRequires time to prepare examples Consumes tokens (higher cost) 3. Chain of Thought (CoT) Chain of Thought (CoT) is a technique that requires the model to explain each step of thinking before giving the final answer.\nAdvantages:\nIncreases accuracy for complex problems Helps debug and understand the model\u0026rsquo;s logic Suitable for problems requiring reasoning How to use:\nAdd the phrase \u0026ldquo;Let\u0026rsquo;s think step by step\u0026rdquo; to the prompt Request the model to explain each step Check the logic before using the result Retrieval Augmented Generation (RAG) What is RAG? Retrieval Augmented Generation (RAG) is an architecture that combines:\nRetrieval: Retrieve information from internal data sources Generation: Use AI model to generate answers RAG Workflow User Query: User poses a question Document Retrieval: System searches for relevant information in knowledge base Context Building: Combines retrieved information with original prompt Generation: AI model generates answer based on complete context Response: Returns result to user Benefits of RAG Accuracy: Increases accuracy with internal data Up-to-date: Always uses the latest information Cost-effective: No need to fine-tune model Security: Control over data sources Common Use Cases Customer support chatbot Internal enterprise Q\u0026amp;A system Document search and summarization Knowledge management systems Other AWS AI Services Amazon Rekognition Amazon Rekognition is an image and video analysis service using machine learning.\nKey Features:\nFace Detection \u0026amp; Recognition: Identify faces Object \u0026amp; Scene Detection: Detect objects and scenes Celebrity Recognition: Recognize celebrities Text in Image: Extract text from images Content Moderation: Filter inappropriate content Use Cases:\nFace authentication system Image classification and search Video surveillance and security Media content analysis Amazon Comprehend Amazon Comprehend is a Natural Language Processing (NLP) service for text analysis.\nKey Features:\nSentiment Analysis: Analyze emotions (positive, negative, neutral) Entity Recognition: Identify names, locations, organizations Key Phrase Extraction: Extract important phrases Language Detection: Detect language Topic Modeling: Classify topics Use Cases:\nAnalyze customer feedback Social media monitoring Content categorization Document classification Key Takeaways About Technology Amazon Bedrock is the optimal solution for deploying Gen AI with many model choices Prompt Engineering is an important skill to maximize AI capabilities RAG helps combine the power of AI with internal enterprise data AWS provides a diverse AI services ecosystem for many different use cases About Implementation Start simple: Use zero-shot prompting for basic tasks Optimize gradually: Apply few-shot or CoT when high accuracy is needed RAG for private data: No need to fine-tune model, save costs Combine services: Leverage multiple AWS AI services for comprehensive solutions About Business Value Reduce costs: Use managed services instead of building infrastructure Increase speed: Deploy quickly with pre-trained models Scalability: AWS automatically scales according to demand Security: AWS ensures security and compliance Event Experience Attending the \u0026ldquo;Generative AI with Amazon Bedrock\u0026rdquo; workshop was an extremely beneficial experience, helping me understand more deeply about Gen AI technology and how to apply it in practice.\nLearning from Experts Speakers with practical experience shared specific case studies about deploying Gen AI in enterprises Learned in detail about Prompt Engineering techniques and how to optimize prompts Better understanding of RAG architecture and how to implement it in real systems Technical Knowledge Gained Mastered prompting techniques: Zero-shot, Few-shot, Chain of Thought Clearly understood RAG workflow and how to integrate with internal knowledge base Learned how to choose the appropriate Foundation Model for each use case Explored other AWS AI services: Rekognition, Comprehend Practical Applications The workshop gave me many ideas to apply in my work:\nBuilding chatbot: Use RAG to create a chatbot with knowledge about company products/services Document processing: Automatically extract and classify information from documents Content generation: Support creating marketing content, technical documentation Customer insights: Analyze customer feedback and sentiment Networking Met and exchanged ideas with professionals in the AI/ML field Shared experiences and best practices with the community Created connections for future collaboration opportunities In conclusion, the event not only provided technical knowledge but also opened up a vision about AI trends and how enterprises can leverage this technology to create competitive advantage.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Configure Internet &amp; NAT Gateway","tags":[],"description":"","content":"Create Internet Gateway In the VPC dashboard, click on Internet gateways Then click on Create internet gateway In the Create internet gateway section, choose a name as you like, then click Create internet gateway and wait for it to be created After the Internet gateway is created, go to the Actions section and click Attach to VPC to attach it to the VPC created in the previous section Create NAT Gateway Create a NAT Gateway located in Public Subnet 1\nAssign an Elastic IP to have a static address to the Internet\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Initialize Amazon RDS","tags":[],"description":"","content":"Initialize Amazon RDS Access RDS Console \u0026gt; Subnet groups \u0026gt; Create DB subnet group\nConfigure Subnet Group:\nName: db-private-group Subnets: Select 2 AZs and choose the correct 2 Private Subnets Go to Databases \u0026gt; Create database Configure Database: Engine options: Microsoft SQL Server (Express Edition)\nTemplates: Free tier\nSettings: Set Master Password (remember it for later use)\nConnectivity:\nVPC: The VPC you created for Web Subnet group: db-private-group Public access: No VPC security group: Select the Security group you created for the database Click Create database "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.5-policy/5.5.2-section2/","title":"Initialize Elastic Beanstalk","tags":[],"description":"","content":"Initialize Elastic Beanstalk We will create the application runtime environment.\nAccess Elastic Beanstalk \u0026gt; Create application Configure Application: App Name: MiniMarket-App Platform: Docker (Amazon Linux 2023) Application code: Select Sample application (To test infrastructure first) Network Configuration (Networking) - Extremely important: VPC: Select the VPC you created for MiniMarket Instance settings: Public IP address: Uncheck Subnets: Select 2 Private Subnets EC2 security groups: Select sg-web-app Capacity: Environment type: Select Load balanced Load balancer network settings: Visibility: Public Subnets: Select 2 Public Subnets Click Create. The system will take approximately 5-7 minutes to initialize. "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"IAM Permissions Attach the AdministratorAccess IAM permission policy to your AWS account for easier workflow.\nNote: Using Administrator permissions is only recommended for Workshop environments to ensure the deployment process is not interrupted. In actual Production environments, the Least Privilege principle must be followed for each service.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Source Code GitHub repository containing valid .NET Core code and Dockerfile.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.6-cleanup/5.6.2-section2/","title":"Setup CodePipeline","tags":[],"description":"","content":"Setup CodePipeline Access CodePipeline \u0026gt; Create pipeline Category: Select Build custom pipeline Settings: Select New service role Source Stage: Select GitHub (via GitHub App) \u0026gt; Connect to GitHub \u0026gt; Select the repository and branch you want to deploy to the cloud Build Stage: Select AWS CodeBuild \u0026gt; Select the MiniMarket-Build project you just created Test Stage: Click Skip test stage\nDeploy Stage:\nProvider: AWS Elastic Beanstalk Application name: MiniMarket-App Environment name: Select the running environment Click Create pipeline If the Deploy step fails with a Permission error, go to the IAM Role of CodePipeline and grant AdministratorAccess-AWSElasticBeanstalk permission\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.7-section7/5.7.2-subsection2/","title":"Setup Firewall (WAF)","tags":[],"description":"","content":"Setup Firewall (WAF) Access WAF \u0026amp; Shield \u0026gt; Protection packs (webs ACLs) \u0026gt; Create protection pack (web ACL) App category: E-commerce \u0026amp; transaction platforms App focus: Both API and web Add resources \u0026gt; Add CloudFront or Amplify resources \u0026gt; Select the CloudFront distribution created in the previous section Choose initial protections \u0026gt; Build your own pack from all of the protections AWS WAF offers \u0026gt; AWS-managed rule group: Add Core rule set (Block bots, bad IPs) Add SQL database (Block SQL Injection) Testing: Access URL: https://[domain]/?id=1 OR 1=1. If you receive a 403 Forbidden error, WAF is working.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Configure Spring Data JPA and connect to database (PostgreSQL/MySQL) Develop Employee management APIs Develop Payroll statistics APIs Tasks to be carried out this week: Day Learning / Research Practice \u0026amp; Code AWS Event Attended 2 - Learn Spring Data JPA - Entity and Repository concepts - Configure application.properties - Create Entity classes - Create Repository interfaces Deploying Java Applications on AWS EC2 3 - Learn about DTO Pattern - Separation of Entity and DTO - Implement API: GET /api/v1/employees - Implement API: GET /api/v1/employees/total 4 - Learn about JPA Query Methods - Implement API: GET /api/v1/employees/active/count - Implement API: GET /api/v1/employees/{id} 5 - Learn payroll business logic - Database schema for payroll - Design payroll tables - Prepare sample data 6 - Learn Java Stream API - groupingBy, summingDouble - Implement API: GET /api/v1/payroll/monthly - Implement API: GET /api/v1/payroll/summary 7 - Learn about task/evaluation business logic - Implement API: GET /api/v1/tasks - Implement API: GET /api/v1/evaluations Week 10 Achievements: Successfully configured Spring Data JPA, connected to PostgreSQL/MySQL database.\nCreated Entity classes (Employee, Payroll, Task, Evaluation) corresponding to database tables.\nCreated Repository interfaces extending JpaRepository, understood built-in methods (findAll, findById, save, delete).\nImplemented 4 Employee management APIs:\nGET /api/v1/employees (retrieve all employees) GET /api/v1/employees/total (count total employees) GET /api/v1/employees/active/count (count active employees) GET /api/v1/employees/{id} (retrieve employee by ID) Implemented 2 Payroll statistics APIs:\nGET /api/v1/payroll/monthly (retrieve monthly payroll list) GET /api/v1/payroll/summary (retrieve total statistics by department) Implemented 2 additional APIs:\nGET /api/v1/tasks (retrieve task list) GET /api/v1/evaluations (retrieve evaluation list) Used Java Stream API to process data, group, and calculate statistics efficiently.\nAttended AWS online event: \u0026ldquo;Deploying Java Applications on AWS EC2\u0026rdquo;.\nView EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Understand attendance business logic (check-in/check-out rules) Integrate Lambda to automate attendance operations Develop Leave Management APIs Tasks to be carried out this week: Day Learning / Research Practice \u0026amp; Code AWS Event Attended 2 - Learn check-in/check-out business rules - Time calculation concepts - Design database schema - Prepare sample data Building Serverless Applications with AWS Lambda 3 - Learn about pagination - Query with date range - Implement API: GET /api/v1/attendance - Implement API: GET /api/v1/attendance/range 4 - Learn about data update rules - Implement API: PUT /api/v1/attendance/{recordId} 5 - Learn about Lambda and API Gateway integration - Trigger concepts (API Gateway, EventBridge) - Create Lambda Function (Python/Node.js) - Integrate with API Gateway 6 - Learn Lambda execution flow - DynamoDB for event logging - Implement POST /api/attendance/start - Implement POST /api/attendance/stop 7 - Learn leave management business logic - Approval workflow (Pending, Approved, Rejected) - Implement API: GET /api/v1/leaves - Implement API: GET /api/v1/leaves/history/{employeeId} - Implement API: POST /api/v1/leaves - Implement API: PUT /api/v1/leaves/{id}/approve - Implement API: PUT /api/v1/leaves/{id}/reject Week 11 Achievements: Understood business rules for check-in/check-out: Automatic calculation of work hours, overtime, late arrival, early departure.\nImplemented 3 Attendance management APIs:\nGET /api/v1/attendance (retrieve attendance list with pagination) GET /api/v1/attendance/range (query by date range) PUT /api/v1/attendance/{recordId} (update attendance record) Created Lambda Function (Python or Node.js), successfully integrated with API Gateway.\nImplemented 2 Lambda endpoints:\nPOST /api/attendance/start (start attendance via Lambda) POST /api/attendance/stop (stop attendance via Lambda) Used DynamoDB to log Lambda execution events.\nImplemented 5 Leave Management APIs:\nGET /api/v1/leaves (retrieve all leave requests) GET /api/v1/leaves/history/{employeeId} (retrieve leave history by employee) POST /api/v1/leaves (create new leave request) PUT /api/v1/leaves/{id}/approve (approve leave request) PUT /api/v1/leaves/{id}/reject (reject leave request) Implemented approval workflow with status: Pending → Approved/Rejected.\nAttended AWS online event: \u0026ldquo;Building Serverless Applications with AWS Lambda\u0026rdquo;.\nView EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Use ElastiCache (Redis) to improve API performance Integrate AWS Secrets Manager for credentials security Develop APIs related to evaluations and support system Tasks to be carried out this week: Day Learning / Research Practice \u0026amp; Code AWS Event Attended 2 - Learn about ElastiCache (Redis) - Cache-Aside Pattern - Create Redis Cluster - Configure Security Group Improving Application Performance with Amazon ElastiCache 3 - Learn Spring Data Redis - RedisTemplate and CacheManager - Integrate Redis into Spring Boot - Implement caching for /payroll/summary API 4 - Learn AWS Secrets Manager - Environment variable management - Store DB credentials in Secrets Manager - Update application.properties to retrieve secrets 5 - Learn about Amazon Rekognition - Face Recognition APIs - Analyze Rekognition Face Recognition - Prepare employee photos AI/ML on AWS – Image \u0026amp; Face Recognition Basics 6 - Learn business logic for evaluations and support tickets - Implement API: GET /api/v1/evaluations/{id} - Implement API: POST /api/v1/evaluations 7 - Complete support system APIs - Swagger documentation - Implement API: GET /api/v1/support-tickets - Implement API: PUT /api/v1/support-tickets/{id}/forward - Implement API: PUT /api/v1/support-tickets/{id}/notify - Generate Swagger/OpenAPI documentation - Review Spring Security and JWT Week 12 Achievements: Created ElastiCache Redis Cluster, configured Security Group to allow connections from Spring Boot application.\nIntegrated Spring Data Redis, implemented Cache-Aside Pattern to cache data from /payroll/summary API, significantly improving response time.\nStored database credentials and API keys in AWS Secrets Manager, updated application.properties to automatically retrieve credentials via AWS SDK.\nAnalyzed Amazon Rekognition APIs for Face Recognition, prepared employee face photos for future features (e.g., face recognition attendance).\nImplemented 2 Evaluation management APIs:\nGET /api/v1/evaluations/{id} (retrieve evaluation by ID) POST /api/v1/evaluations (create new evaluation) Implemented 3 Support System APIs:\nGET /api/v1/support-tickets (retrieve support ticket list) PUT /api/v1/support-tickets/{id}/forward (forward ticket to relevant department) PUT /api/v1/support-tickets/{id}/notify (send notification to employee) Generated complete API documentation using Swagger/OpenAPI, including all endpoints (Employee, Payroll, Attendance, Leave, Evaluation, Support).\nReviewed the entire Spring Security and JWT authentication system, ensuring all APIs are properly secured.\nAttended AWS online events: \u0026ldquo;Improving Application Performance with Amazon ElastiCache\u0026rdquo; and \u0026ldquo;AI/ML on AWS – Image \u0026amp; Face Recognition Basics\u0026rdquo;.\nView EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Enterprise HR Management System Comprehensive HR Management Solution for Modern Enterprises 1. Executive Summary Enterprise HR Management System is an integrated HR management solution designed for mid-sized enterprises in Vietnam, supporting 100-500 employees. The system automates the entire HR workflow from profile management, attendance tracking, payroll calculation to performance evaluation. This is an in-house project developed by the team, focusing on MVP with optimized costs under $100/month in the initial phase (100 employees), utilizing AWS serverless architecture with Lambda, API Gateway, DynamoDB to ensure high performance and low costs.\n2. Problem Statement Current Issues Vietnamese enterprises use Excel or legacy HR software, causing time waste and errors. Manual processes (attendance, payroll) are not integrated. No automated approval workflows. Difficult to manage detailed permissions. Weak reporting, not real-time. High costs for SAP, Workday solutions. Proposed Solution The system uses AWS Serverless Architecture to optimize costs:\nCompute: AWS Lambda (pay-per-use, no idle costs). API: API Gateway REST API. Database: DynamoDB (on-demand billing). Cache: ElastiCache Redis (cache.t3.micro) - optional for phase 2. Authentication: AWS Cognito (free tier \u0026lt;50K MAU). Storage: S3 for documents, CloudFront CDN. CI/CD: GitHub Actions for automated deployment. Monitoring: CloudWatch (free tier). Security: Route 53, WAF (cost-optimized rules), IAM Roles. Key Features Single Sign-On (Google, Microsoft 365). Detailed RBAC (Admin, Manager, Employee, Payroll Officer). Check-in/out with GPS validation. Automated payroll calculation with flexible formulas. Approval workflows (leave, salary adjustment). Mobile app (React Native) for attendance. Real-time reporting dashboard. Comprehensive audit logs. Benefits Save 70% of manual HR processing time. Reduce 90% of data entry errors. Cost only $45-70/month for 100 employees (90% cheaper than SAP/Workday). In-house development - no outsourcing costs. 3. Solution Architecture Here is the cloud architecture diagram of the system:\nAWS Services Used AWS Service Primary Function AWS Lambda Backend API logic (Node.js 20.x) API Gateway REST API endpoints, request validation Amazon DynamoDB NoSQL database (on-demand billing) AWS Cognito Authentication, SSO (Google/Microsoft), JWT tokens Amazon S3 Document storage (CV, contracts, payslips) CloudFront CDN for static assets and S3 Route 53 DNS management AWS WAF (optional Phase 2) API protection CloudWatch Logs, monitoring (free tier) Secrets Manager API keys, credentials Component Design Authentication Layer Cognito User Pools with JWT (RS256). Lambda authorizer for API Gateway. Optional MFA (SMS/TOTP) - Phase 2. API Layer AWS Lambda functions (Node.js) deployed via GitHub Actions. API Gateway REST API with resource-based routing. Rate limiting (10 requests/second). CORS configured for web/mobile. Business Logic (Lambda Functions) Employee management (CRUD, contracts, skills). Attendance tracking (check-in/out, GPS validation). Leave management (requests, approvals, balance). Payroll engine (salary calculation, tax, insurance). Performance reviews (KPI tracking). Email notifications (SES free tier). Data Layer - DynamoDB Tables Users - GSI on email Employees - GSI on department_id Departments AttendanceLogs - GSI on employee_id + date LeaveRequests - GSI on employee_id + status PayrollRecords - GSI on employee_id + month Approvals - GSI on approver_id + status Storage Layer S3 Standard for new documents (\u0026lt;30 days). S3 Lifecycle → Glacier Deep Archive (\u0026gt;90 days). Presigned URLs for secure upload/download. CloudFront distribution for static web hosting. Frontend Next.js 14 (React 18) + TypeScript - Static export. Material-UI components. Hosted on CloudFront + S3 (no server cost). Mobile app: React Native (Expo) with AsyncStorage. CI/CD Pipeline GitHub Actions workflow: Build Lambda functions → ZIP packages Deploy to Lambda via AWS CLI Update API Gateway configurations Deploy frontend to S3 Automated Jest unit tests. 4. Technical Implementation Phase 1: MVP Core (Month 1-2) Month 1: AWS setup (Cognito, DynamoDB tables, S3, Lambda). Authentication + Login UI. Employee CRUD APIs + admin dashboard. Month 2: Attendance check-in/out APIs with GPS. Mobile app MVP (React Native). Leave request workflow. Basic reporting dashboard. Phase 2: Payroll \u0026amp; Automation (Month 3-4) Month 3: Payroll calculation engine (Lambda). Payslip generation (PDF via Lambda layer). Approval workflows. Month 4: Email notifications (SES). Audit logging to DynamoDB. Export reports (CSV). Performance optimization. Phase 3: Advanced Features (Month 5-6) Performance review module. Training tracking. Advanced analytics dashboard. Security hardening. Load testing \u0026amp; optimization. User training \u0026amp; documentation. Tech Stack Component Technology/Service Backend Node.js 20.x, AWS Lambda, AWS SDK v3 Database DynamoDB (single-table design pattern) Frontend Next.js 14, React 18, TypeScript, Material-UI v5 Mobile React Native (Expo), AsyncStorage Infrastructure as Code AWS SAM / Serverless Framework CI/CD GitHub Actions 5. Roadmap \u0026amp; Milestones Month Phase Key Deliverables 1-2 MVP Core Auth, Employee management, Attendance mobile app 3-4 Payroll \u0026amp; Automation Payroll engine, approval workflows, notifications 5-6 Advanced \u0026amp; Launch Analytics, performance reviews, UAT, go-live 6. Budget Estimation Monthly AWS Costs (Phase 1: 100 employees, ~5,000 API calls/day) Serverless Architecture - Cost Optimized Service Configuration Cost/Month AWS Lambda 150K invocations, 512MB, 500ms avg $0 ↳ Free tier: 1M requests + 400K GB-seconds/month (Within free tier) API Gateway 150K REST API requests/month $0.15 ↳ $3.50 per million after first 1M (free tier year 1) DynamoDB On-demand, 5GB storage, 1M reads, 500K writes $3.50 ↳ Storage: $1.25/GB ($6.25) + Reads: $0.25/M + Writes: $1.25/M S3 Storage 20GB documents (100 users) $0.46 S3 Requests 20K PUT, 100K GET/month $0.14 S3 Glacier (archive) 10GB old documents $0.10 CloudFront 10GB transfer, 200K requests $1.00 Route 53 1 hosted zone + 1M queries $0.90 CloudWatch Logs 2GB logs/month $0 ↳ (First 5GB free) (Within free tier) Secrets Manager 2 secrets $0.80 SES (email) 500 emails/month $0.05 Cognito \u0026lt;50K MAU $0 ↳ (Free tier) (Within free tier) Data Transfer OUT 5GB to internet $0.45 Contingency (10%) Buffer $0.75 TOTAL AWS/MONTH (100 users) ~$8.30 Costs When Scaling to 200 Users (Phase 2) Service Changes Cost/Month Lambda 300K invocations (still in free tier) $0 API Gateway 300K requests $0.30 DynamoDB 10GB, 2M reads, 1M writes $9.50 S3 + CloudFront 40GB storage, 20GB transfer $2.50 Route 53, Secrets, SES, Transfer (similar) $2.20 ElastiCache Redis cache.t3.micro (optional) $12.50 AWS WAF Basic protection (optional) $7.00 Contingency $3.40 TOTAL (200 users, with cache + WAF) ~$37.40 TOTAL (200 users, without cache/WAF) ~$17.90 Costs When Scaling to 500 Users (Phase 3) | Lambda + API Gateway | 750K invocations | $3.50 | | DynamoDB | 25GB, 5M reads, 2.5M writes | $32.50 | | S3 + CloudFront + Transfer | 100GB storage, 50GB CDN | $7.50 | | ElastiCache Redis | cache.t3.small | $25.00 | | AWS WAF | 2 rules | $8.00 | | Route 53, Secrets, SES, misc | | $3.00 | | Contingency | | $8.00 | | | | | | TOTAL (500 users) | | ~$87.50 |\nHosting Cost Summary by Phase Phase Users Cost/Month Cost/Year Phase 1 MVP 100 $8-12 ~$100-150 Phase 2 Growth 200 $18-38 ~$220-450 Phase 3 Scale 500 $88-95 ~$1,050 Development Costs (In-house team - NO outsourcing cost) Assumption: In-house team already has fixed salaries, only AWS and tools costs counted\nItem Cost AWS hosting (6 months dev/staging @ $5/mo) $30 GitHub Pro (team of 5) $0 ↳ (Can use free tier) Domain name (.com) $12/year Third-party libraries (optional) $0 TOTAL DEVELOPMENT COST ~$42 Note: Personnel costs NOT included as this is an in-house team with fixed salaries\nAnnual Operating Costs (post go-live) Item Cost/Year AWS Hosting (Phase 1: 100 users) $100-150 Third-party services (SMS for MFA - optional) $100 Domain renewal $12 TOTAL OPERATING/YEAR (Phase 1) ~$212-262 ROI Analysis (In-house project) Initial Investment:\nSetup + Dev tools: ~$42 AWS (6 months dev): ~$30 Total initial: ~$72 First Year Operating Costs:\nPhase 1 (6 months, 100 users): $60 Phase 2 (6 months, 200 users): $150 Total Year 1: ~$210 Total Year 1 Cost: ~$282\nSavings vs Alternatives:\nSAP SuccessFactors: $8-15/user/month = $9,600-18,000/year BambooHR: $6-10/user/month = $7,200-12,000/year Manual Excel: 1 FTE HR admin = $12,000/year Year 1 Savings: $6,918 - $17,718 Year 1 ROI: 2,454% - 6,281% 🚀\n7. Risk Assessment \u0026amp; Mitigation Risk Impact Probability Mitigation DynamoDB costs spike Medium Low On-demand billing, CloudWatch alarms at $30 threshold Lambda cold starts Low Medium Keep functions warm, optimize bundle size \u0026lt;1MB API Gateway rate limits Medium Low Default 10K req/s sufficient, implement caching Vendor lock-in (AWS) Medium High Use Serverless Framework for portability Team learning curve Low Medium Start with 1-2 Lambda functions, expand gradually Cost Optimization Best Practices Lambda: Bundle size \u0026lt;1MB, reuse connections, avoid cold starts. DynamoDB: Single-table design, use GSIs carefully, on-demand billing. S3: Lifecycle policies to Glacier, presigned URLs, CloudFront caching. API Gateway: Response caching (30-60s), throttling. CloudWatch: Log retention 7 days, filter unnecessary logs. 8. Expected Outcomes Technical Improvements 85% HR processes automated. Real-time dashboard with data \u0026lt; 5 seconds old. \u0026lt; 1s API response time (P95) with Lambda. 70% employees use mobile app. Zero server maintenance. Infinite scalability with serverless. Business Value HR team reduces 60% manual workload. Employee satisfaction increases 40% (self-service). 100% audit trail for compliance. Payroll accuracy 99.5%. Cost savings $6,900-17,700/year vs alternatives. Operating cost only $8-12/month for 100 users. Long-term Vision Scale to 500 users at ~$88/month cost. Integrate AI/ML (AWS Bedrock) for predictive analytics. Multi-branch operations. Potential SaaS product. 9. Conclusion HR Management System with Serverless Architecture provides:\n✅ Ultra-low cost: Only $8-12/month for 100 users Phase 1\n✅ No upfront cost: ~$72 setup, no outsourcing costs\n✅ Massive ROI: Save $6,900-17,700/year vs alternatives\n✅ Scalable: Pay-as-you-go, auto-scale to 500+ users\n✅ Zero maintenance: Serverless = no server management\n✅ Fast development: 6 months MVP → production\nThis is an ideal solution for startups/SMEs with in-house teams wanting to build a modern HR system without large investments.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"17/11/2025 DevOps Mindset Speaker Information Truong Quang Tinh - AWS Community Builder, Platform Engineer at TymeX What is DevOps? DevOps is not just a job position or a set of tools, but a culture and mindset in software development. A DevOps engineer is someone who understands both the Development (Dev) and Operations (Ops) worlds, acting as a bridge between these two sides to create a more efficient software development and deployment process.\nBefore DevOps, common issues included:\nDevelopers wrote code and \u0026ldquo;threw it over the wall\u0026rdquo; to Ops Ops faced deployment issues but didn\u0026rsquo;t understand the code Slow release cycles with many errors Lack of communication and mutual understanding DevOps was created to break down these barriers, creating a continuous and collaborative process.\nDevOps Culture - Cultural Elements 1. Collaboration Break down boundaries between teams Shared responsibility for products Cross-functional teams 2. Automation Maximize process automation Minimize human errors Code can be reviewed, tested, rolled back 3. Monitoring \u0026amp; Measurement Must have systems for evaluation and monitoring Collect metrics for data-driven decisions Timely alerts when issues arise 4. Continuous Learning Learn from mistakes, no blaming Share knowledge within team Encourage experimentation 5. Tooling Version Control: Git, GitHub, GitLab CI/CD: Jenkins, GitLab CI, GitHub Actions IaC: Ansible, Terraform Containerization: Docker, Kubernetes Monitoring: Prometheus, Grafana, CloudWatch Next-Generation DevOps DevOps is evolving with new trends:\nPlatform Engineering: Building Internal Developer Platform (IDP), self-service for developers GitOps: Git as single source of truth, declarative infrastructure AIOps: AI/ML in monitoring, predicting issues, automated remediation DevSecOps: Shift left security, automated security testing Comparing Roles Role Focus Key Skills Cloud Engineer Design \u0026amp; manage cloud infrastructure AWS/Azure/GCP, networking, security DevOps Engineer Automate CI/CD, bridge Dev-Ops Scripting, CI/CD tools, containerization, IaC SRE Reliability, availability, performance Programming, monitoring, incident response Platform Engineer Build internal platform Infrastructure, developer experience, API design DORA Metrics - Measuring DevOps Effectiveness 1. Deployment Frequency Elite: Multiple deploys per day High: Once per week to once per month 2. Lead Time for Changes Elite: Less than one hour High: One day to one week 3. Change Failure Rate Elite: 0-15% High: 16-30% 4. Time to Restore Service Elite: Less than one hour High: Less than one day DevOps Journey Roadmap Phase 1: Foundation Linux fundamentals, networking basics Version control (Git) Programming/Scripting (Python, Bash) Phase 2: Automation CI/CD: Jenkins, GitHub Actions IaC: Terraform, Ansible Containerization: Docker, Kubernetes Phase 3: Monitoring \u0026amp; Observability Logging: ELK Stack Monitoring: Prometheus + Grafana Alerting \u0026amp; incident management Phase 4: Cloud \u0026amp; Scalability AWS services: EC2, S3, Lambda, ECS, EKS Auto-scaling, load balancing Caching \u0026amp; database optimization Phase 5: Security \u0026amp; Advanced DevSecOps: Security scanning, secret management SRE practices: SLI/SLO/SLA, chaos engineering Platform Engineering \u0026amp; GitOps Key Takeaways About DevOps Mindset DevOps is culture, not just tools Automation is key: Reduce risks and increase efficiency Collaboration over silos: Break down barriers between teams Measure everything: Data-driven decisions About Technical Infrastructure as Code: Version control everything CI/CD is mandatory: Automated pipeline Monitoring \u0026amp; Observability: Can\u0026rsquo;t improve what you don\u0026rsquo;t measure Start small, iterate: Gradual improvement, no need to be perfect from the start About Career DevOps is a journey: Continuous learning Broad knowledge: Dev, ops, cloud, security Soft skills matter: Communication and collaboration are important Multiple paths: SRE, Platform Engineering, Cloud Architecture Event Experience Attending the \u0026ldquo;DevOps Mindset\u0026rdquo; talk by Truong Quang Tinh was an invaluable experience, helping me gain deep insights into DevOps and career development roadmap.\nLearning from a Practitioner Tinh is an AWS Community Builder and Platform Engineer at TymeX with hands-on experience Real-world examples from work helped understand challenges and solutions Shared best practices and lessons learned Important Insights DevOps Culture: Not just tools but changing mindset and working methods Automation: \u0026ldquo;Automate everything\u0026rdquo; to reduce human errors DORA metrics: How to measure DevOps transformation effectiveness Next-gen trends: Platform Engineering, GitOps, AIOps Clear Roadmap Detailed guidance on skills needed at each stage Understanding differences between DevOps, SRE, Cloud Engineer, Platform Engineer Career path direction based on interests Applying to Work Automation: Write scripts for repetitive tasks Learn IaC: Start with Terraform Setup monitoring: Implement basic monitoring Practice CI/CD: Build pipeline with GitHub Actions Join community: Participate in AWS Community In conclusion, the talk inspired about DevOps mindset - collaboration, automation, continuous improvement. This is the foundation to become a great DevOps Engineer.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.5-policy/5.5.3-section3/","title":"Configure Environment Variables","tags":[],"description":"","content":"Configure Environment Variables To enable the application to connect to the Database and Redis, we don\u0026rsquo;t hardcode in the code but use Environment Variables.\nGo to Beanstalk Environment \u0026gt; Configuration \u0026gt; Updates, monitoring, and logging \u0026gt; Edit\nScroll down to the Environment properties section\nAdd the following variables:\nVariable 1:\nName: ConnectionStrings__DefaultConnection Value: Server=sql-shop-db….rds.amazonaws.com;Database=MiniMarketDB;User Id=admin;Password=YOUR PASSWORD;TrustServerCertificate=True; Variable 2:\nName: ConnectionStrings__RedisConnection Value: webapp.redis.cache…:6379 Variable 3:\nName: VnPay__IPNUrl Value: https://[cloudfrontdomain].cloudfront.net/Payment/VnPayIPN Variable 4:\nName: VnPay__ReturnUrl Value: https://[cloudfrontdomain].cloudfront.net/Payment/VnPayReturn Click Apply. The server will restart to receive the new configuration. "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.3-s3-vpc/5.3.3-route-table/","title":"Configure Route Table","tags":[],"description":"","content":"Create Route Table Click on Route tables in the VPC dashboard Create 2 Route Tables, Public and Private Public Route Table: For the Public Route Table, in the Routes section, click on Edit routes\nPoint 0.0.0.0/0 to the Internet Gateway\nIn the Subnet associations section, assign both Public Subnets Private Route Table: For the Private Route Table, we will point 0.0.0.0/0 to the NAT Gateway In the Subnet associations section, assign both Private Subnets Separating Route Tables ensures that Databases in the Private Subnet are never directly exposed to the Internet.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Initialize ElastiCache Redis","tags":[],"description":"","content":"Initialize ElastiCache Redis Access ElastiCache \u0026gt; Subnet groups \u0026gt; Create subnet group\nName: redis-private-group Subnets: Select 2 Private Subnets Go to Redis OSS caches \u0026gt; Create cache In the Cluster settings screen: Engine: Select Redis OSS Deployment option: Select Node-based cluster Creation method: Select Cluster cache (Configure and create a new cluster) Cluster mode: Select Disabled (Simple mode, 1 Shard) In the Location screen:\nLocation: AWS Cloud Multi-AZ: Uncheck (Enable) Note: Disable this feature to save costs for Lab environment Auto-failover: Uncheck (Enable) In the Cache settings screen:\nEngine version: Keep default (e.g., 7.1) Port: 6379 Node type: Select t3 line \u0026gt; Select cache.t3.micro Number of replicas: Enter 0 (We only need 1 primary node, no backup nodes) In the Connectivity screen: Network type: IPv4 Subnet groups: Select Choose existing subnet group \u0026gt; Select the newly created redis-private-group In the Advanced settings screen (Important): Encryption at rest: Enable (Default) Encryption in transit: Uncheck (Disable) Reason: Disabling transit encryption simplifies connection from .NET code in the internal VPC environment without requiring complex SSL certificate configuration Selected security groups: Select Manage \u0026gt; select sg-redis-cache (Uncheck default) Scroll to the bottom and click Create Get Connection Information The initialization process will take approximately 5-10 minutes\nWhen the status changes to Available (Green):\nClick on the Cluster name (webapp or the name you set) In the Overview tab, find the Primary endpoint section Copy this connection string (Example: webapp.xxxx.cache.amazonaws.com) This endpoint will be used to configure the ConnectionStrings__RedisConnection environment variable for Elastic Beanstalk in the following steps.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.3-s3-vpc/","title":"Network Infrastructure Setup","tags":[],"description":"","content":"💡 Overview In this initial phase, we will build a solid and secure network foundation (VPC) for the Human Resource Management (HRM) System. Ensuring secure network infrastructure is a prerequisite for protecting sensitive HR data and complying with security regulations.\nThe HRM network architecture will be designed to include:\nVPC (Virtual Private Cloud): An isolated network region on AWS. Public Subnet: For components that need to communicate directly with the Internet (e.g., Internet-facing Load Balancer, NAT Gateway, CloudFront/WAF Endpoints). Private Subnet: Dedicated to core resources requiring high security, not directly accessible from the Internet (e.g., EC2/Webapp Service, Amazon RDS for PostgreSQL, AWS ElastiCache, DynamoDB). NAT Gateway: Allows application servers (App Servers) in Private Subnets to access the Internet (e.g., to download security updates, pull Docker Images, or connect to AWS services outside the VPC) while keeping internal IP addresses hidden. VPC Endpoint (PrivateLink): Establishes a private and secure connection between your VPC and other AWS services (such as Amazon S3, DynamoDB) without going through the Internet, enhancing security for uploading face check-in images. 📝 Implementation Content Initialize VPC \u0026amp; Subnet Configure Internet \u0026amp; NAT Gateway Configure Route Table "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Automating Budget Management Across Multi-Account Environments This blog demonstrates how to automate budget management across multiple AWS accounts using an event-driven architecture. The solution enables centralized budget management with automated email notifications, allowing organizations to set and enforce account-specific budgets from a central management account. Key technologies include Amazon DynamoDB for storing budget values, AWS Lambda for propagating budget configurations via AWS Systems Manager Parameter Store, and AWS Budgets for real-time spending monitoring. The article provides detailed deployment steps using CloudFormation templates and explains how to handle budget alert states and implement future enhancements like AWS Budget Actions and ServiceNow integration for automated cost optimization.\nBlog 2 - Guidance for a media lake on AWS This blog introduces a comprehensive solution for building a centralized media lake on AWS to manage large volumes of digital media files scattered across multiple Amazon S3 buckets. The guidance provides a reference architecture that creates a unified search interface, allowing users to search media files across all S3 locations simultaneously. It features automated event-driven media processing pipelines using a drag-and-drop interface, AI-powered natural language search capabilities, and seamless integration with AWS services. The solution can be deployed via three methods: AWS CloudFormation template, local deployment using AWS CDK, or integration with existing CI/CD pipelines, making it flexible for different organizational needs.\nBlog 3 - Dynamic configuration updates in .NET using Parameter Store and Secrets Manager This blog explores an advanced approach to managing configuration and secrets in .NET applications using AWS Systems Manager Parameter Store and AWS Secrets Manager. The solution demonstrates how to reference Secrets Manager secrets through Parameter Store using the /aws/reference/secretsmanager/ format, enabling unified access to both configuration and sensitive data. Key features include implementing dynamic configuration reloading without application restarts using the IOptionsMonitor pattern, environment separation through hierarchical Parameter Store structures, and secure storage with proper encryption and access controls. The article provides step-by-step C# code examples showing how to set up extension methods, configuration models, and dependency injection for seamless integration with .NET 8 applications.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"29/11/2025 AWS IAM Security Best Practices Speaker Information Huynh Hoang Long - AWS Expert Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud Journey What is IAM? IAM (Identity and Access Management) is AWS\u0026rsquo;s identity and access management service, allowing management of roles with specific permissions assigned to specific users, enhancing system security.\nKey Components:\nUsers: Individual users Groups: User groups Roles: Roles with specific permissions Policies: Policies defining permissions IAM Best Practices 1. Principle of Least Privilege Don\u0026rsquo;t grant excessive permissions, only grant minimum necessary permissions.\nReduce risk when account is compromised Limit damage if incidents occur Developers only need deploy permissions, not database deletion rights 2. Delete Root Access Keys Delete root access keys immediately after setup.\nRoot has the highest permissions Don\u0026rsquo;t use root for daily work Store root credentials securely 3. Avoid Wildcards (*) Don\u0026rsquo;t use \u0026ldquo;*\u0026rdquo; as it grants full permissions.\n// Avoid {\u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;} // Use instead {\u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-bucket/*\u0026#34;} 4. Use AWS SSO AWS SSO instead of regular IAM users.\nCredentials auto-reset (15 minutes - 36 hours) Enhanced security, centralized management Single Sign-On (SSO) SSO allows single login to access multiple systems.\nBenefits:\nEasy management of multiple roles and accounts No need to login again when switching apps Activating SSO simultaneously activates AWS Organizations Multi-Factor Authentication (MFA) MFA is an additional authentication step, protecting accounts even when passwords are compromised.\nTypes:\nVirtual MFA: Google/Microsoft Authenticator, Authy Hardware: YubiKey, Gemalto tokens SMS OTP (less secure) Best practices: Mandatory MFA for all users, prioritize Virtual/Hardware MFA.\nPassword Policies Strong password policies:\nLength: 12-14 characters Complexity: Uppercase, lowercase, numbers, special characters Expiration: 60-90 days No reuse of last 5 passwords Service Control Policies (SCP) SCP defines maximum permissions in account/OU.\nDoesn\u0026rsquo;t grant permissions, only limits them Applied at AWS Organizations level Overrides all IAM policies Permission Boundaries Permission Boundaries set maximum permission limits for IAM entities.\nUse cases:\nAllow developers to create roles but not exceed boundary Prevent privilege escalation Credentials Rotation AWS Secrets Manager auto-rotation:\nCreate new credentials Test credentials Apply to app Delete old credentials Best practices: Rotate access keys at least every 90 days, use IAM roles when possible.\nIAM Access Analyzer IAM Access Analyzer analyzes and monitors access permissions:\nDetects resources shared with external entities Validates policy syntax Suggests least-privilege policies Common findings:\nPublic/externally shared S3 buckets Externally accessible KMS keys IAM roles assumable from external accounts Key Takeaways About IAM Security Least Privilege: Grant minimum permissions only No Root Keys: Don\u0026rsquo;t use root for daily work Avoid Wildcards: Specify actions and resources clearly Enable MFA: Mandatory for all users About Authentication AWS SSO: Centralized management, auto-rotate credentials MFA: Critical security layer Strong Passwords: Complex, periodic rotation Temporary Credentials: Prioritize IAM roles About Governance SCP: Limit permissions at organization level Permission Boundaries: Prevent privilege escalation Credentials Rotation: Automated with Secrets Manager Access Analyzer: Continuous monitoring Event Experience Attending the \u0026ldquo;AWS IAM Security Best Practices\u0026rdquo; workshop helped me gain deep understanding of AWS security.\nLearning from Experts Long and Anh shared hands-on IAM experience Case studies about security incidents and prevention Best practices from large enterprises Important Insights Least Privilege: Mandatory requirement, not just theory Defense in Depth: Multiple security layers (MFA, SCP, Permission Boundaries) Automation: Auto credential rotation reduces risks Key Tools AWS SSO: Modern enterprise authentication IAM Access Analyzer: Audit and validate policies Secrets Manager: Automated credentials management Applying to Work Audit permissions: Review and tighten policies Enable MFA: Deploy for all users Setup SSO: Migrate from IAM users Credential rotation: Use Secrets Manager Access Analyzer: Monitor security findings In conclusion, the workshop emphasized security mindset - least privilege, multiple security layers, and continuous monitoring.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.4-s3-onprem/","title":"Deploy Data Layer","tags":[],"description":"","content":"Overview Data is the most important asset of any system. Therefore, we will set up the Data Layer for MiniMarket with the criteria: Maximum Security and High Performance.\nWe will deploy two core services:\nAmazon RDS (Relational Database Service): Using SQL Server to store business data (Products, Orders, Users). The database will be placed in a Private Subnet to prevent direct access from the Internet. Amazon ElastiCache (Redis): Using Redis as an in-memory cache to store login sessions and reduce query load on the main database. Content Setup Security Groups for DB \u0026amp; Cache Initialize Amazon RDS (SQL Server) Initialize Amazon ElastiCache (Redis) "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in 5 events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS GenAI Builder - AI-Driven Development Lifecycle (AI-DLC)\nDate \u0026amp; Time: 09:00, October 03, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: Learning about AI-Driven Development Lifecycle, working with AI as a companion in software development, Prompt Engineering techniques and Mob Construction.\nEvent 2 Event Name: Generative AI with Amazon Bedrock\nDate \u0026amp; Time: 08:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: Learning about Amazon Bedrock, Foundation Models, Prompting techniques (Zero-Shot, Few-Shot, Chain-of-Thought) and practical applications of Gen AI.\nEvent 3 Event Name: DevOps Mindset\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: Understanding DevOps culture, the role of DevOps Engineer, important cultural elements (Collaboration, Automation, Monitoring) and CI/CD pipeline.\nEvent 4 Event Name: AWS IAM Security Best Practices\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: Learning about IAM (Identity and Access Management), security best practices such as Least Privilege, removing Root Access Keys, using MFA and Password Policy.\nEvent 5 Event Name: AWS GameDay - Agentic AI GenAI Builder Club\nDate \u0026amp; Time: 14:30, November 21, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: Competition to build and deploy Agent AI using Python on AWS platform. Team achieved 3rd place with 269,000 points by discovering the core logic in the final 15 minutes.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"AWS GameDay - Agentic AI GenAI Builder Club 🚀 Event Overview On November 21, 2025, I participated in AWS GameDay - Agentic AI GenAI Builder Club organized by AWS. This was a highly competitive event where teams had to demonstrate their skills in developing and deploying solutions using Generative AI technology on the AWS platform.\nThe event revolved around building and optimizing AI Agents to handle a series of requirements and test cases.\n💻 Main Tasks Teams were provided with an initial code environment, and the mission was to:\nDevelop with Python: Use Python to write logic for AI Agents. Deploy Code: Continuously deploy modified code to the AWS environment so that test cases automatically run and score. Debug and Optimize: Based on scores and feedback from the system, quickly debug and optimize Agents to pass increasingly complex test cases. Scoring: Points are awarded when Agents successfully pass cases. 🏆 Team Achievement: Spectacular Comeback Our team participated and achieved an impressive result: Third Place overall!\nFinal Score: 269,000 points. Most Notable Moment: The final 15 minutes of the competition. Throughout the early period, our team only achieved around 64,000 points and was outside the Top 10. However, by discovering the core logic to handle a critical series of test cases, we quickly deployed the solution and our score skyrocketed to 269,000 points, securing a Top 3 finish overall. This was an excellent experience, not only technically (Python, Deploy, AWS GenAI) but also in teamwork and handling time pressure.\n📸 Event Photos "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.5-policy/","title":"Deploy Application","tags":[],"description":"","content":"Overview After having the network and data infrastructure, the next step is to deploy the .NET Core application source code to the Cloud. Instead of manually managing individual EC2 virtual servers, we will use the Platform-as-a-Service (PaaS) platform AWS Elastic Beanstalk.\nObjectives of this module:\nContainerization: Package the MiniMarket application into a Docker Container to ensure a consistent runtime environment (Dev = Prod) Deployment: Deploy the Container to Elastic Beanstalk. The system will automatically provision EC2, configure Load Balancer and Auto Scaling Group Connectivity: Configure the application to securely connect to RDS and Redis through Environment Variables Content Package application with Docker Initialize Elastic Beanstalk Environment Configure Database \u0026amp; Redis connection Connect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/","title":"Workshop","tags":[],"description":"","content":" Deploy Human Resource Management System on AWS Cloud Overview This workshop demonstrates a comprehensive Cloud Migration journey, showcasing how to migrate and deploy a Human Resource Management (HRM) System from on-premises infrastructure to AWS Cloud using modern Cloud-Native architecture patterns.\nThe HRM system is a core platform built on Java Spring Boot, designed to comprehensively manage employee records, attendance tracking, and other HR tasks. This workshop focuses on migrating the application from an on-premise environment to a Cloud-Native architecture on AWS, with particular emphasis on ensuring sensitive data security and scalability.\nThe migration strictly adheres to the pillars of the AWS Well-Architected Framework: Security, Reliability, Performance Efficiency, and Cost Optimization.\nSolution Architecture Overview The HRM project uses a Microservices architecture and carefully selected AWS services to maximize security for sensitive HR data.\nArchitecture Components:\nCompute \u0026amp; Logic Layer:\nAWS Elastic Beanstalk (or ECS): Leverages the Docker platform to package and deploy Java Spring Boot applications. This simplifies infrastructure management and automates Auto Scaling and Elastic Load Balancing. AWS Lambda \u0026amp; API Gateway: Used to process asynchronous tasks (e.g., report generation, salary calculation) or lightweight APIs, optimizing costs and performance. Data \u0026amp; Persistence Layer:\nAmazon RDS for PostgreSQL: Used as the primary relational database (RDB) to store employee records, salary information, and sensitive data. Placed in Private Subnet to maintain the highest security. Amazon DynamoDB: Used for unstructured, high-speed data (e.g., storing record change history, transaction logs). Amazon S3: Used to store and distribute static assets such as profile pictures and face check-in images with superior durability. Security \u0026amp; Authentication:\nAmazon Cognito: Provides user identity management services, ensuring authentication and access authorization for employees and management. AWS Secrets Manager: Integrated to securely store and manage API keys, database credentials, and Tokens for session creation or external integration. DevOps \u0026amp; Monitoring:\nAWS CodePipeline \u0026amp; CodeBuild: Automates the CI/CD (Continuous Integration/Continuous Deployment) process from source code to production environment. Amazon CloudWatch \u0026amp; SNS: Monitors system health and sends alerts when issues occur. Workshop Objectives By completing this workshop, you will:\nBuild secure network infrastructure with VPC, Subnets, and Security Groups Deploy data layer with RDS PostgreSQL and ElastiCache Redis in Private Subnets Containerize and deploy Java Spring Boot application using Docker and Elastic Beanstalk Implement automated CI/CD pipeline with CodeBuild and CodePipeline Optimize performance using CloudFront CDN and S3 for static assets Enhance security with AWS WAF to protect against common web attacks Set up monitoring and alerting with CloudWatch and SNS Learn proper resource cleanup procedures to avoid unexpected costs Content Introduction to HRM System Prerequisites Setup Network Infrastructure Deploy Data Layer Deploy Application Automate CI/CD Optimization \u0026amp; Security Monitoring \u0026amp; Operations Clean up Resources "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.6-cleanup/","title":"Automate CI/CD","tags":[],"description":"","content":"Overview In a Cloud Native environment, manual deployment is risky and time-consuming. This module will guide you through building a fully automated CI/CD (Continuous Integration / Continuous Deployment) workflow.\nThe workflow operates as follows:\nSource: Developer pushes code to GitHub Build: AWS CodePipeline detects changes and triggers AWS CodeBuild. CodeBuild packages the Docker Image and pushes it to the Amazon ECR repository Deploy: Pipeline automatically instructs Elastic Beanstalk to update to the latest version from ECR without causing service interruption Content Create Build Project with AWS CodeBuild Setup AWS CodePipeline clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AWS Vietnam Co., Ltd. from August 12, 2025 to December 9, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the human resource management software development project, through which I improved my teamwork skills, programming thinking, data analysis, and project management.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ☐ ✅ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Areas for Improvement Enhance Discipline and Compliance:\nNeed to be more serious in adhering to regulations, schedules, and standard company procedures to build a professional work ethic.\nDevelop In-depth Problem-Solving Thinking:\nRather than just stopping at temporary troubleshooting, need to cultivate the habit of root cause analysis to provide optimal and long-term solutions.\nPerfect Communication and Interpersonal Skills:\nNeed to learn to communicate more tactfully and sensitively in daily situations and enhance emotional intelligence (EQ) to coordinate more harmoniously with colleagues.\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.7-section7/","title":"Optimization &amp; Security","tags":[],"description":"","content":"Optimization \u0026amp; Security Overview A Production system needs not only to \u0026ldquo;run\u0026rdquo; but also to \u0026ldquo;run fast\u0026rdquo; and \u0026ldquo;securely\u0026rdquo;. In this section, we will fine-tune the MiniMarket architecture to achieve higher performance and enhanced security.\nImplementation items: Offloading Static Assets: Transfer all product images from Web Server to Amazon S3 and distribute via Amazon CloudFront (CDN) to increase global page load speed and reduce server load Security Hardening: Deploy AWS WAF (Web Application Firewall) in front of CloudFront to protect the application from common attacks such as SQL Injection and XSS Content Configure S3 and CloudFront Setup AWS WAF "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe environment at FCJ is very dynamic and open. The work atmosphere is professional yet not restrictive. Team members always maintain a positive attitude, ready to share and support each other regardless of time. The workspace is well-organized and inspires creativity. However, if there were more regular bonding activities, the connection between members would be even stronger.\n2. Support from Mentor / Team Admin\nI highly appreciate the dedication of the Mentor and Admin team. Mentors not only provide technical guidance but also cultivate a problem-solving mindset, encouraging me to explore solutions before offering suggestions, which helps me retain knowledge longer and understand more deeply. The Admin team closely follows up and provides timely support with procedures and resources, ensuring the internship process runs smoothly.\n3. Relevance of Work to Academic Major\nThe internship content closely aligns with the foundational knowledge I learned at university, while expanding into many technologies and practical processes that textbooks haven\u0026rsquo;t yet covered. This was a great opportunity to systematize my knowledge and fill gaps in practical experience.\n4. Learning \u0026amp; Skill Development Opportunities\nBeyond professional knowledge, I significantly improved soft skills such as: time management, teamwork, and communication in a corporate environment. Being exposed to standardized work processes (such as Agile/Scrum or project management workflows) is valuable preparation for future work.\n5. Company Culture \u0026amp; Team Spirit\nThe \u0026ldquo;Supportive\u0026rdquo; culture impressed me most. Everyone treats each other with respect and a constructive spirit. Even as an intern, I felt my voice and contributions were recognized. The team\u0026rsquo;s spirit of \u0026ldquo;work hard, play hard\u0026rdquo; also helps reduce pressure during peak periods.\n6. Internship Policies / Benefits\nThe company\u0026rsquo;s support policies are quite good and transparent. The internship allowance is reasonable, and especially the time flexibility helps students easily balance their studies. Internal sharing/training sessions are also a valuable \u0026ldquo;knowledge benefit.\u0026rdquo;\nAdditional Questions What did you find most satisfying during your internship?\nWhat I found most satisfying was the growth in my work mindset. From only knowing how to follow instructions, I learned to proactively analyze problems and seek solutions. Additionally, the enthusiasm of the Mentors helped me overcome initial uncertainties.\nWhat do you think the company should improve for future interns?\nOverall, the process is already quite good. I only suggest the program could add a list of reference materials (reading list) or a more detailed learning roadmap from the first week so new interns can proactively study before entering real projects.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nAbsolutely yes. This is an ideal environment for those who want to experience real-world pressure while still receiving attentive guidance. FCJ not only teaches skills but also instills professional work attitudes and practices.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nI suggest adding regular \u0026ldquo;1-1 Review\u0026rdquo; sessions every 2 weeks so interns can receive more detailed feedback on their strengths and weaknesses, allowing timely adjustments during the internship.\nWould you like to continue this program in the future?\nGiven the value received, I very much hope to have the opportunity to continue or become a full-time employee if I meet the company\u0026rsquo;s requirements.\nThank you to the FCJ team for creating a valuable playground and memorable experience during my student life. I wish the program continues to grow and welcome many more generations of talented interns!!\n"},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.8-section8/","title":"Monitoring &amp; Operations","tags":[],"description":"","content":"Monitoring \u0026amp; Operations Overview A Production system cannot be considered complete without Monitoring and Alerting capabilities. You cannot sit and watch the screen 24/7 to check if the Server is still alive.\nIn this module, we will set up monitoring and alerting for the MiniMarket system using AWS operational management services:\nAmazon CloudWatch: Collect metrics from EC2, RDS, ELB Amazon SNS (Simple Notification Service): Notification delivery service. We will use it to send emails to administrators when the system encounters issues We will set up a CloudWatch Alarm to monitor the Web Server\u0026rsquo;s CPU. If CPU exceeds 70% (a sign of overload or attack), the system will automatically trigger SNS to send an emergency alert email.\nContent Setup CloudWatch \u0026amp; SNS Alerts "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/5-workshop/5.9-section9/","title":"Clean up resources","tags":[],"description":"","content":"Clean up resources Overview Congratulations on successfully migrating and deploying the Human Resources Management (HRM) System to AWS Cloud infrastructure!\nHowever, our work is not yet complete. The final and most important step to protect your budget is to Clean up Resources deployed during the Workshop.\nThe services we used such as Amazon RDS for PostgreSQL, NAT Gateway, Elastic Load Balancer (ELB), AWS ElastiCache (Redis), and EC2 Instances running the Java Spring Boot application are all charged hourly, whether you use the application or not. If you forget to delete them, the monthly bill can be very high.\nWe will perform a Decommissioning (deactivation/removal) process of the system in the proper sequence to ensure that no resources are left behind causing hidden costs.\nContent Safe resource deletion procedure "},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://huuvinh05926.github.io/AWS_Proposal/en/tags/","title":"Tags","tags":[],"description":"","content":""}]